{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mike's Site","text":"<p>Storage for my notes on Linux, K8s, and other things.</p>"},{"location":"docker_commands/","title":"commands","text":""},{"location":"docker_commands/#ansible","title":"Ansible","text":""},{"location":"docker_commands/#go-to-ansible-docker-file","title":"go to ansible docker file","text":"<p>cd /home/linux-joe/git/dockerfiles/ansible</p>"},{"location":"docker_commands/#build-the-container","title":"Build the container","text":"<p>sudo docker build . -t py_ansible</p>"},{"location":"docker_commands/#start-ansible-container","title":"Start ansible container","text":"<p>sudo docker compose run ansible</p>"},{"location":"docker_commands/#in-the-container","title":"in the container","text":"<p>ansible-playbook -i nuc_inventory playbooks/hello_world.yml ansible-playbook -i nuc_inventory playbooks/</p>"},{"location":"docker_commands/#sudoers-and-using-become","title":"Sudoers and using become","text":"<p>ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/shutdown.yml ansible-playbook -i nuc_inventory --ask-vault-pass --extra-vars '@passwd.yml' playbooks/nuc_config_playbook.yml ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/nuc_config_playbook.yml ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/k8s_prereqs_playbook.yml ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/k8s_prereqs_playbook.yml --tags \"waka\" ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/k8s_resources.yml -vv</p>"},{"location":"docker_commands/#siteyml-and-limit-to-2-nodes","title":"site.yml and limit to 2 nodes","text":"<p>ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' site.yml -l nuc1,nuc4  -vv ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' site.yml -l nuc1,nuc4  -vv --check ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' site.yml -vv</p>"},{"location":"docker_commands/#kubectl-commands","title":"Kubectl commands","text":"<p><code>kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code> INGRESS_EXTERNAL_IP=<code>kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code></p> <ul> <li>metal lb tutorial: https://www.adaltas.com/en/2022/09/08/kubernetes-metallb-nginx/</li> </ul> <p>kubectl -n metallb-system describe controller-7499d4584d-28ccw kubectl -n metallb-system get secret metallb-webhook-cert -ojsonpath='{.data.ca.crt}' kubectl -n metallb-system get secret metallb-webhook-cert -ojsonpath='{.data.ca.crt}' | base64 -d kubectl -n metallb-system get secret metallb-webhook-cert -ojsonpath='{.data.ca.crt}' | base64 -d &gt; caBundle.pem kubectl -n metallb-system get secret webhook-server-cert -ojsonpath='{.data.ca.crt}' kubectl -n metallb-system get secret webhook-server-cert -ojsonpath='{.data.ca.crt}' | base64 -d kubectl -n metallb-system get secret webhook-server-cert -ojsonpath='{.data.ca.crt}' | base64 -d &gt; caBundle.pem kubectl create deploy nginx --image nginx kubectl expose deploy nginx --port 80 --type LoadBalancer kubectl delete deployment nginx kubectl delete pod -n kube-flannel -l app=flannel kubectl delete pod -n metallb-system kubectl delete pod -n metallb-system -l app=metallb kubectl delete pod -n metallb-system -l metallb kubectl delete svc nginx kubectl describe crd ipaddresspools.metallb.io kubectl describe nodes kubectl describe nodes nuc1 kubectl describe pod test-pod-nginx kubectl describe pods kubectl describe pods --all-namespaces kubectl describe pods -n default kubectl describe pods -n default nginx-7854ff8877-j292l kubectl describe pods -n default nginx-7854ff8877-pfvhq kubectl describe pods -n kube-flannel kubectl describe pods -n metallb-system kubectl describe service nginx kubectl describe svc kubectl describe svc kubernetes-dashboard kubectl describe svc metallb kubectl describe svc metallb-system kubectl describe svc metallb-webhook-service kubectl describe svc metallb-webhook-service -n metallb-system kubectl edit configmap kubectl get all kubectl get all --all-namespaces kubectl get crd kubectl get endpoints -n metallb-system kubectl get endpoints -n metallb-system~ kubectl get events --sort-by=.metadata.creationTimestamp kubectl get logs -n kube-flannel kubectl get logs -n kube-flannel kube-flannel-ds-8755f kubectl get namespace kubectl get nodes kubectl get nodes --help kubectl get nodes --show-label kubectl get nodes --show-labels kubectl get nodes -o wide kubectl get pod --all-namespaces kubectl get pods kubectl get pods --all-namespaces kubectl get pods -n default kubectl get pods -n metallb-system kubectl get secrets kubectl get secrets -n metallb-system kubectl get service -n metallb-system kubectl get service -n metallb-system  metallb-webhook-service kubectl get service -n metallb-system  webhook-service kubectl get svc kubectl get svc --help kubectl get svc nginx kubectl get svc nginx -o kubectl get svc nginx -o json kubectl get svc nginx -o jsonpath kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip} kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' kubectl get svc nginx -o jsonpath='{.status}' kubectl get validatingwebhookconfiguration metallb-webhook-configuration -ojsonpath='{.webhooks[0].clientConfig.caBundle}' kubectl get validatingwebhookconfiguration metallb-webhook-configuration -ojsonpath='{.webhooks[0].clientConfig.caBundle}' kubectl get validatingwebhookconfiguration metallb-webhook-configuration -ojsonpath='{.webhooks[0].clientConfig.caBundle}' | base64 -d kubectl list svc kubectl log nginx-7854ff8877-j292l kubectl logs -n kube-flannel kube-flannel-ds-8755f kubectl logs nginx-7854ff8877-j292l kubectl logs test-pod-nginx kubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8080:443 kubectl proxy kubectl run -i --tty --rm test-pod-nginx --image=nginx --restart=Never --namespace default</p>"},{"location":"docker_commands/#expose-a-running-service-with-the-load-balancer","title":"expose a running service with the load balancer","text":"<p>k -n kubernetes-dashboard expose service kubernetes-dashboard-web --type=LoadBalancer --name=kubernetes-dashboard-svc</p> <p>enable kube dashboard with https so token works: (edit the the kong proxy service to have a nodeport and go directly to i, circumvent the loadbalancer ip)</p> <p>https://github.com/kubernetes/dashboard/issues/9066#issuecomment-2254511968 dashboard IP with kong modified: https://192.168.1.47:32260/ dashboard via svc external ip: 192.168.1.241:8000</p>"},{"location":"docker_commands/#start-here","title":"START HERE","text":"<p>Why the difference?  Kong is circumventing tls, but how, it is able to go directly to web? Need to understand traffic flow into k8s pods.</p>"},{"location":"docker_commands/#helm","title":"helm","text":"<p>100  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/   102  /usr/local/bin/helm list --output=yaml   103  /usr/local/bin/helm list --output=yaml --filter kube-dashboard   104  /usr/local/bin/helm list --output=yaml --filter kubernetes-dashboard   105  helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard   106  helm delete kubernetes-dashboard --namespace kubernetes-dashboard</p>"},{"location":"docker_commands/#ansible-vault","title":"Ansible vault","text":"<p>To create a new encrypted data file, run the following command:</p> <p><code>ansible-vault create foo.yml</code></p> <p>To edit an encrypted file in place, use the ansible-vault edit command. This command will decrypt the file to a temporary file and allow you to edit the file, saving it back when done and removing the temporary file:</p> <p><code>ansible-vault edit foo.yml</code></p>"},{"location":"todo_list/","title":"TO DO","text":"<ul> <li>cards game - make interactive game for users, write out sequence diagram for initializing a game and test</li> <li>k8s - expose mkdocs_notes on a local endpoint</li> <li>k8s - write notes on manfiests, control plane, network</li> <li>linux - write notes on IPC, context switching, memory/virtual memory</li> <li>Finish Ruby book on OOP</li> <li></li> </ul>"},{"location":"Kube/Install_k8s_dash_board_metallb/","title":"To expose the Kubernetes Dashboard using MetalLB, you need to follow a few steps. Here's a guide on how to set it up","text":""},{"location":"Kube/Install_k8s_dash_board_metallb/#step-1-install-metallb-if-not-already-installed","title":"Step 1: Install MetalLB (if not already installed)","text":"<p>First, ensure that MetalLB is installed and running on your Kubernetes cluster.</p> <ol> <li>Apply the MetalLB manifest:</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.3/manifests/metallb.yaml\n</code></pre> <ol> <li>Create a ConfigMap for MetalLB to use the address pool <code>my-test-app</code> that you created:</li> </ol> <p>Here's an example <code>ConfigMap</code> for MetalLB. Replace the IP range with the appropriate range you set for the <code>my-test-app</code> pool:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: metallb-config\n  namespace: metallb-system\ndata:\n  config: |\n    address-pools:\n    - name: my-test-app\n      protocol: layer2\n      addresses:\n      - 192.168.1.240-192.168.1.250  # Use the range from your address pool\n</code></pre> <p>Apply this config with:</p> <pre><code>kubectl apply -f metallb-config.yaml\n</code></pre>"},{"location":"Kube/Install_k8s_dash_board_metallb/#step-2-deploy-the-kubernetes-dashboard","title":"Step 2: Deploy the Kubernetes Dashboard","text":"<p>If you haven't already deployed the Kubernetes Dashboard, you can deploy it using the following command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n</code></pre>"},{"location":"Kube/Install_k8s_dash_board_metallb/#step-3-create-a-service-to-expose-the-dashboard","title":"Step 3: Create a Service to Expose the Dashboard","text":"<p>Next, you need to expose the Kubernetes Dashboard through a LoadBalancer service that MetalLB can assign an IP to.</p> <p>Create a <code>Service</code> definition for the Kubernetes Dashboard as follows:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  selector:\n    k8s-app: kubernetes-dashboard\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9090\n  type: LoadBalancer\n</code></pre> <p>Apply the service:</p> <pre><code>kubectl apply -f dashboard-service.yaml\n</code></pre>"},{"location":"Kube/Install_k8s_dash_board_metallb/#step-4-verify-the-loadbalancer-ip","title":"Step 4: Verify the LoadBalancer IP","text":"<p>MetalLB will assign an IP from the <code>my-test-app</code> address pool to the <code>kubernetes-dashboard</code> service.</p> <p>You can check the assigned IP with:</p> <pre><code>kubectl get svc -n kubernetes-dashboard\n</code></pre> <p>You should see the <code>EXTERNAL-IP</code> populated with an IP address from the <code>my-test-app</code> pool.</p>"},{"location":"Kube/Install_k8s_dash_board_metallb/#step-5-access-the-dashboard","title":"Step 5: Access the Dashboard","text":"<p>Once the service is exposed and the IP address is assigned, you can access the Kubernetes Dashboard by visiting the assigned IP address in your browser.</p> <p>If you use port 80 in the service, just navigate to:</p> <pre><code>http://&lt;assigned-ip&gt;\n</code></pre>"},{"location":"Kube/Install_k8s_dash_board_metallb/#step-6-set-up-the-access-token","title":"Step 6: Set Up the Access Token","text":"<p>To log in to the Kubernetes Dashboard, you'll need to create a ServiceAccount and ClusterRoleBinding for the necessary permissions:</p> <pre><code>kubectl create serviceaccount dashboard-sa -n kubernetes-dashboard\nkubectl create clusterrolebinding dashboard-sa-binding \\\n  --clusterrole=cluster-admin \\\n  --serviceaccount=kubernetes-dashboard:dashboard-sa\n</code></pre> <p>Get the token for the <code>dashboard-sa</code> ServiceAccount:</p> <pre><code>kubectl -n kubernetes-dashboard create token dashboard-sa\n</code></pre> <p>This token can be used to log in to the Kubernetes Dashboard.</p>"},{"location":"Kube/Install_k8s_dash_board_metallb/#summary","title":"Summary","text":"<ol> <li>Install and configure MetalLB.</li> <li>Deploy the Kubernetes Dashboard.</li> <li>Create a LoadBalancer service to expose the dashboard using the <code>my-test-app</code> address pool.</li> <li>Retrieve the assigned IP address.</li> <li>Log in to the dashboard with the generated access token.</li> </ol> <p>Let me know if you need further clarification!</p>"},{"location":"dump/mkdocs_notes/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"dump/mkdocs_notes/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"dump/mkdocs_notes/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files\n</code></pre> <p>Good links: Feature overview material for mkdocs: https://jameswillett.dev/getting-started-with-material-for-mkdocs/#share-on-socials</p> <p>Material for MKdocs: https://squidfunk.github.io/mkdocs-material/getting-started/</p>"},{"location":"dump/prompts/","title":"Useful gpt prompts","text":""},{"location":"dump/prompts/#code-review","title":"code review","text":"<p>Provide a code review for my blackjack game. Offer input or suggestions for improvment. Explain what to improve, how to improve it, and why it is incorrect/inefficient. </p>"},{"location":"linux/What%20happens%20when%20you%20press%20ctrl%5Ec/","title":"What happens when you press CRTL^C?","text":"<p>When you press Ctrl-C on your local machine, the kernel sends a SIGINT signal (Signal interrupt)  to the foreground process group of the terminal. The terminal driver is responsible for translating key presses into signals and passing them to the correct process group. The default action for SIGINT is to terminate the process, but the process can catch the signal and handle it in a custom way.</p> <p>When you press Ctrl-C in a terminal, the terminal emulator writes an ASCII character (^C) to the master device. The kernel translates that into sending a SIGINT signal to the foreground process group with the corresponding controlling terminal.  This is actually a default terminal setting. You can run <code>stty -a</code> and see that the default is <code>intr = ^C</code>;, meaning ^C or ETX is the \"SIGINT\" character.</p> <p><code>bash line-numbers highlight=3 linux-joe@x1:$ stty -a speed 38400 baud; rows 24; columns 80; line = 0; intr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = &lt;undef&gt;; eol2 = &lt;undef&gt;; swtch = &lt;undef&gt;; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R;...</code></p> <p>A more complex example would be how Ctrl-C works through an interactive SSH session. Interactive SSH sessions allocate a pty on the server side. The client side pty is set to raw mode, meaning that the client side kernel will not translate ETX into SIGINT. Instead, the client side kernel passes the ETX along to the slave. In this case, the ssh client process takes that ETX and passes it along to the server sshd process. If the server sshd pty is not in raw mode, then the server's kernel will translate that ETX into a SIGINT to its foreground process group. This is how Ctrl-C sends SIGINT to the process running on the server instead of killing your client side SSH and leaving you hanging.</p>"},{"location":"linux/What%20happens%20when%20you%20press%20ctrl%5Ec/#components","title":"Components","text":""},{"location":"linux/What%20happens%20when%20you%20press%20ctrl%5Ec/#signals","title":"Signals","text":"<p>Signals are a protocol for interrupting and closing programs from the outside. There are a few different types of signals, and they all do different things.</p> <p>Some signals, like <code>SIGKILL</code> and <code>SIGSTOP</code>, cannot be caught or ignored by the program. They will always kill or stop the program.</p> <p>Other signals can be caught by the program and handled in a custom way. For example, Ctrl+C in the terminal almost always sends SIGINT, however, a program can catch <code>SIGINT</code> and do something other than exit when it receives that signal.</p>"},{"location":"linux/What%20happens%20when%20you%20press%20ctrl%5Ec/#terminal-this-needs-editing","title":"terminal - This needs editing","text":"<p>what is a pty? A pty is a pseudo-terminal. It is a pair of devices that provide a bidirectional communication channel. One device is the master, and the other is the slave. The master device is responsible for translating key presses into signals and passing them to the slave device. The slave device is responsible for reading key presses and writing output.</p> <p>When you run a program in a terminal, the terminal emulator creates a pty pair and runs the program with the slave device as its controlling terminal. The terminal emulator then reads key presses from the master device and writes them to the slave device. The program reads key presses from the slave device and writes output to the master device. The terminal emulator reads output from the master device and displays it on the screen.</p>"},{"location":"linux/contex_switching/","title":"Context Switching in Operating Systems: A Detailed Explanation","text":"<p>How does the kernel handle context switching efficiently.</p>"},{"location":"linux/contex_switching/#scenario-context-switch-between-task-a-and-task-b","title":"Scenario: Context Switch Between Task A and Task B","text":"<p>I walk through a context switch in a Linux system where the kernel switches from Task A to Task B</p> <p>Focus:</p> <ul> <li>Kernal's role in managing context switches</li> <li>Isolation and security between tasks</li> </ul>"},{"location":"linux/contex_switching/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<p>Key components:</p> <p>Scheduler: what is the scheduler in linux?: The scheduler is an operating system component that determines which tasks run on the CPU and when the tasks run. It is responsible for managing the CPU\u2019s time efficiently and fairly. The scheduler uses various algorithms to decide what is fair (i.e. which task to run next), based on factors like task priority, time slices (i.e. alloted time), and other scheduling policies.</p> <p>Process control block: The Process Control Block (PCB) is a data structure used by the operating system to manage information about a process. It contains information such as the process ID, process state (i.e. stack pointer), the next instruction to execute (i.e. program counter), CPU registers, and other details needed to manage and control the process.</p> <p>Program Counter: The program counter (PC) is a register in the CPU that stores the address of the next instruction to be executed. It is a key component in the context switch process, as it determines where the CPU should resume execution for a seamless transition. During a context switch, the Program counter is saved and restored to ensure that the task resumes execution correctly.</p> <p>Stack pointer: The stack pointer (SP) is a register in the CPU that points to the top of the stack. The stack is used to store function calls, local variables, and return addresses. During a context switch, the stack pointer is saved and restored to ensure that the task resumes execution correctly.</p>"},{"location":"linux/contex_switching/#1-triggering-the-context-switch","title":"1. Triggering the Context Switch","text":"<ul> <li> <p>When it happens: A context switch is triggered when the operating system decides that the currently running task (Task A) has either used up its time slice, is waiting for some I/O operation, or is blocked for some other reason (e.g., waiting for a resource to become available). Switching also happen when a higher-priority task becomes ready to run or when an interrupt occurs (like a hardware interrupt from a device).</p> </li> <li> <p>The Role of the Scheduler: The kernel\u2019s scheduler decides when a context switch happens. The scheduler\u2019s role is to ensure fair distribution of CPU time among tasks and to respect their priority levels. It decides which task should run next based on factors like task priority, time slices, and other internal scheduling algorithms (e.g., round-robin, priority-based scheduling).</p> </li> </ul>"},{"location":"linux/contex_switching/#2-saving-task-as-state","title":"2. Saving Task A\u2019s State","text":"<ul> <li> <p>Pausing Task A: The kernel halts Task A\u2019s execution when the scheduler decides to switch. The key here is to preserve the task\u2019s state so that it can continue seamlessly from where it left off.</p> </li> <li> <p>What gets saved: The kernel saves the following state of Task A:</p> </li> <li>CPU Registers: These are small, fast, temporary storage areas used by the CPU to hold values that the task is currently working with (like data, intermediate calculations, pointers to memory, etc.). They are also known as processor registers or memory registers<ul> <li>Examples of CPU registers include the program counter (PC), stack pointer (SP), and general-purpose registers like RAX, RBX, RCX, etc.</li> <li>The registers hold critical information about the task\u2019s execution state</li> <li>Saving and restoring these registers is essential for the task to resume correctly</li> </ul> </li> <li>Each task has its own set of registers, so when switching, the current set of registers needs to be saved to avoid overwriting the task's data.</li> <li>See here for more info on CPU Registers</li> <li>Program Counter (PC): The program counter stores the address of the next instruction that the CPU will execute. This allows the task to resume from the exact point it was interrupted.</li> <li>Stack Pointer (SP): The stack pointer tracks the task\u2019s current position on its stack. The stack is used to store function calls, local variables, and return addresses. This ensures that when Task A resumes, its function calls and local variables are intact.</li> <li> <p>Additional Context: In some cases, the kernel may also save the task\u2019s floating-point or vector registers if the task uses advanced CPU features like SIMD instructions (e.g., for mathematical computations).</p> </li> <li> <p>Where the state is saved: The saved context is typically stored in memory within task_struct (or thread_info) structures. These are kernel-managed data structures that represent each task. Each task has its own <code>task_struct</code>, which contains information like the saved CPU registers, process IDs, the task\u2019s scheduling details, and more.</p> </li> </ul>"},{"location":"linux/contex_switching/#to-do-what-are-the-commands-to-see-the-task-struct","title":"TO DO: ^^ What are the commands to see the task struct?? ^^","text":""},{"location":"linux/contex_switching/#3-choosing-task-b-to-run","title":"3. Choosing Task B to Run","text":"<ul> <li>Scheduler\u2019s Decision: After saving Task A\u2019s state, the scheduler selects the next task to run. In this case, it chooses Task B.</li> <li>The decision could be based on Task B\u2019s priority (if it\u2019s a higher-priority task) or because Task B has become ready to run (for example, after waiting for some resource).</li> <li> <p>The scheduler uses scheduling policies to decide which task gets the CPU next, balancing fairness with efficiency. Common algorithms include round-robin scheduling, priority-based scheduling, and multilevel feedback queues.</p> </li> <li> <p>Choosing the Next Task Efficiently: The scheduler maintains a list (or queue) of ready tasks. It selects the next task based on priority and other factors, minimizing wasted CPU time. The kernel uses preemptive multitasking, meaning the scheduler can interrupt and switch tasks at any time (even within the middle of a task\u2019s execution).</p> </li> </ul>"},{"location":"linux/contex_switching/#4-restoring-task-bs-state","title":"4. Restoring Task B\u2019s State","text":"<ul> <li>Loading Task B\u2019s State: The kernel now restores Task B\u2019s state, allowing it to resume execution. This includes:</li> <li>Restoring CPU Registers: The kernel loads the saved register values from Task B\u2019s state into the CPU registers. These registers include temporary data (e.g., values for ongoing calculations or state tracking) that Task B was working with before being paused.</li> <li>Restoring Program Counter (PC): The program counter is updated to the saved address where Task B left off. This is crucial for ensuring that Task B resumes at the correct place in its execution flow, without skipping any instructions or repeating previous ones.</li> <li> <p>Restoring Stack Pointer (SP): The kernel also updates the stack pointer to where Task B\u2019s stack was when it was paused. This ensures that Task B\u2019s function calls and local variables are restored correctly, and that it continues its execution on the correct stack frame.</p> </li> <li> <p>Isolation: Each task has its own isolated memory space. The kernel ensures that one task cannot interfere with the memory or state of another task. This isolation is fundamental for security and stability. It prevents tasks from accessing each other\u2019s data, which could lead to data corruption or security vulnerabilities. In a multitasking environment, tasks should be isolated, and the kernel provides mechanisms like virtual memory and memory protection to achieve this.</p> </li> <li> <p>Switching Between Tasks Securely: To prevent malicious or poorly-behaved tasks from manipulating other tasks\u2019 data, the kernel enforces strict memory protections and process boundaries. Virtual memory allows each task to believe it has its own private memory, even though multiple tasks may be running on the same physical machine.</p> </li> </ul>"},{"location":"linux/contex_switching/#5-context-switch-completion","title":"5. Context Switch Completion","text":"<ul> <li> <p>Task B Resumes Execution: After restoring Task B\u2019s state, the kernel gives control of the CPU to Task B. Task B begins executing from the point where it left off, as if it had been running continuously.</p> </li> <li> <p>Efficient Execution: The kernel\u2019s job is to ensure that the context switch itself is minimal in overhead. It strives to switch tasks as efficiently as possible, reducing the time spent switching tasks. Context switches need to be quick, or else the system would waste too much time switching between tasks instead of actually running them.</p> </li> </ul>"},{"location":"linux/contex_switching/#6-task-b-running-and-returning-to-task-a","title":"6. Task B Running and Returning to Task A","text":"<ul> <li>Task B Runs: Task B continues to run until one of the following happens:</li> <li>It completes its execution.</li> <li>It gets blocked (waiting for I/O or some other event).</li> <li> <p>It consumes its time slice, and the scheduler decides to switch to another task (possibly back to Task A).</p> </li> <li> <p>Returning to Task A: If Task A needs to be resumed, the kernel repeats the context switch process. Task A\u2019s state is restored, and it begins executing from the point where it left off.</p> </li> </ul>"},{"location":"linux/contex_switching/#key-points-in-the-context-switch-process","title":"Key Points in the Context Switch Process","text":"<ol> <li>State Saving and Restoring: The operating system saves the current state (CPU registers, program counter, stack pointer) of the running task and restores the saved state of the new task.</li> <li>Task Isolation and Security: The kernel isolates each task\u2019s memory and state to ensure security. Virtual memory and memory protection mechanisms prevent tasks from interfering with each other.</li> <li>Efficient Task Scheduling: The kernel uses algorithms to efficiently decide which task to run next. The goal is to minimize the overhead of context switching, so the CPU spends more time running tasks and less time switching between them.</li> <li>Memory Management: Each task operates in its own virtual address space, and the kernel manages memory so that one task cannot corrupt the memory of another task.</li> <li>Preemptive Multitasking: The kernel uses preemptive multitasking, meaning the scheduler can interrupt a running task and switch to another at any point. This enables fair sharing of the CPU and ensures that tasks with higher priority or urgency get to run when needed.</li> </ol>"},{"location":"linux/contex_switching/#in-summary","title":"In Summary","text":"<p>A context switch is a fundamental mechanism that allows the kernel to efficiently manage multiple tasks running on a single CPU. The process involves saving the current task\u2019s state, selecting the next task, and restoring its state so that it can continue running. This switch happens quickly and securely, with mechanisms in place to ensure that tasks are isolated from each other to prevent interference. The kernel ensures that context switches are done efficiently to minimize the overhead and ensure the system can run many tasks simultaneously.</p> <p>I cannot provide direct links, but I can suggest some online resources that commonly have useful diagrams, visualizations, and flowcharts related to context switching. You can visit the following websites and look for related content:</p> <ol> <li>Linux Kernel Documentation    The official Linux kernel documentation has detailed descriptions of process management, including context switching.</li> <li> <p>Link: https://www.kernel.org/doc/html/latest/</p> </li> <li> <p>Operating Systems: Three Easy Pieces    This book, available for free online, is an excellent resource for understanding operating system concepts, including process scheduling and context switching. The book often provides diagrams and flowcharts to explain key concepts.</p> </li> <li> <p>Link: https://pages.cs.wisc.edu/~remzi/OSTEP/</p> </li> <li> <p>GeeksforGeeks (GFG)    GeeksforGeeks provides tutorials on operating system concepts like process management and context switching. They often include flowcharts and diagrams to explain these topics.</p> </li> <li> <p>Link: https://www.geeksforgeeks.org/</p> </li> <li> <p>Wikipedia (Context Switching)    Wikipedia articles often include diagrams and visual aids along with detailed explanations of operating system concepts. The article on context switching usually includes relevant diagrams.</p> </li> <li> <p>Link: https://en.wikipedia.org/wiki/Context_switch</p> </li> <li> <p>Visualgo    Visualgo provides visualizations for many computer science concepts, including scheduling and process management, which can help illustrate how context switching works.</p> </li> <li> <p>Link: https://visualgo.net/en</p> </li> <li> <p>YouTube    Many YouTube channels focused on computer science and operating systems have animated explanations of context switching with accompanying diagrams. Search for terms like \"context switching animation\" or \"operating system context switch.\"</p> </li> <li> <p>OSDev Wiki    The OSDev Wiki contains detailed articles about operating system development, including context switching. It sometimes includes diagrams and visual explanations to help understand how the kernel manages processes.</p> </li> <li>Link: https://wiki.osdev.org/Main_Page</li> </ol> <p>By visiting these resources, you'll find plenty of diagrams and additional materials that explain context switching in an easily understandable way.</p>"},{"location":"linux/networking/","title":"Networking","text":""},{"location":"linux/networking/#etchosts-file","title":"/etc/hosts file","text":"<ul> <li>ip-hostname pairs</li> <li>list of ip addresses and hostnames - provides hostname resolution to an ipv4 or ipv6 address</li> <li>built-in system dns bypass, has maximum priority</li> <li>A match found in the /etc/hosts file will be used before any DNS entry. If the name searched is found in the file (like localhost), no DNS resolution is performed at all as the IP is known.</li> </ul> etc/hosts<pre><code>192.168.1.15 server-a\n192.168.1.16 server-b\n192.168.1.17 server-c\n192.168.1.18 server-d\n</code></pre>"},{"location":"linux/networking/#resolvconf","title":"Resolv.conf","text":"<ul> <li>/etc/resolv.conf specifies the nameservers used for DNS resolution by the host. If you are using DHCP, this file is automatically populated with DNS record issued by DHCP server.</li> <li>It specifies the nameservers in order of search preference for resolver lookups, where it will actually use the DNS protocol for resolving the hostnames.</li> </ul> /etc/resolv.conf<pre><code>nameserver 127.0.0.53\noptions edns0 trust-ad\nsearch .\n</code></pre>"},{"location":"linux/networking/#etcnsswitchconf","title":"/etc/nsswitch.conf","text":"<ul> <li>name services switch</li> <li>Lists the order of sources that must be used by various system library lookup functions. For example, <code>hosts</code> sets the order of hostname resolution to <code>files (aka - /etc/hosts/)</code> then <code>dns (aka - /etc/resolv.conf)</code>.</li> </ul> /etc/nsswitch.conf<pre><code>passwd:         files systemd sss\ngroup:          files systemd sss\nshadow:         files systemd sss\ngshadow:        files systemd\n\nhosts:          files mdns4_minimal [NOTFOUND=return] dns mymachines\nnetworks:       files\n</code></pre>"},{"location":"linux/operating_systems_three_easy_pieces/ch_13_address_spaces/","title":"ch 13 address spaces","text":"<p>Goal is to leave proceses in memory while switching between them in order to allow the OS to implement time sharing efficiently. Don't save memory to disk, it is too slow.</p> <p>Address space is an easy to use abstraction of physical memory that the OS creates in conjunction with the hardware.</p> <p>The address space contains all the memory state of the running program. The code/instructions, stack to track function call chain and local vars, and the heap, used for user managed dynamically allocated memory (malloc'ed), all exists in the address space.  Stack grows up from the limit of the address space, heap grows down.</p> <p>OS has to make sure that when a program tries to load an instruction at address 0, it does not go to the actual physical address 0 but translates the virtual address 0 to some arbitrary address</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/","title":"Chapter 2 Introduction to Operating Systems","text":"<p>A running program executes instructions.</p> <p>When a program runs, millions of times each second, the processor fectches instructions from memory, decodes it and executes it until the program completes.</p> <p>The opreating system is the software that manages the system/hardware correctly and efficiently in an easy to use manner.</p> <p>The OS does this through a technique called virtualization. The OS provides an interface/API to allow users to tell the OS what to do. Every OS has several hundred system calls  to run programs, access memory, devices, and files.</p> <p>Because virtualization allows many programs to run concurrently and access their own instructions and data, the OS is also known as a resource manager.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#virtualizing-the-cpu","title":"Virtualizing the CPU","text":"<p>Turning a single CPU into the illusion that the system has a large number of virtual cpus allowing multiple programs to run at once is one of the primary jobs of the operating system.</p> <p>Policies are the way the operating system decides the priority of which program to run when and how to allocate resources.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#virtualizing-memory","title":"Virtualizing Memory","text":"<p>Physical memory is the actual RAM in the system. Memory is an array of bytes, to read memory one must specify the address to access the data stored there. To write or update memory, one must also specify the data to be written to the address.</p> <p>Memory is accessed all the time the program is running. A running program keeps all of its data structures in memory. Each instruction of the program is stored in memory too, so memory is accesed on each instruction fetch.</p> <p>Each running program appears to have its own private memory instead of sharing the same physical memory with all other prorams. This is done through an OS technique called virtualizing memory. Each process accesses its own private virtual address space which the OS maps onto the physical memory of the machine. Physical memory is a shared resource that is managed by the OS.</p> <p>Concurrency - refers to the host of problems that arise and must be addressed when working on many things at once (concurrently) in the same program. Concurrency is how the OS juggles many things at once, first running one process then another.</p> <p>Threads - a function of a program that runs in the same memory space as other functions, with more than one active at a time. When many threads are working in the same memory space all at once how can we build a correctly working program?  This is problem of concurrency.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#persistence","title":"Persistence","text":"<p>System memory like RAM stores values in a volatile manner - when power goes away or the system crashes the data in memory is lost. We need hardware and software to store data persistently.</p> <p>The hardware comes in the form of input/output (I/O) devices, a hard drive or ssd for example.</p> <p>The operating system sofware that manages the disk is called the file system. It stores files the user creates on disk. The file system is the part of the OS responsible for managing persistent data.</p> <p>System calls to save a file include <code>open()</code> which opens the file and creates it, <code>write()</code> to write data to the file, and <code>close()</code> to close the file. These system calls are routed to the file system which handles the requests and returns a error code to the user.</p> <p>Device drivers are how the OS actually wites to the disk. The OS provides a standard way to access devices through systems calls.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#design-goals","title":"Design goals","text":"<p>The OS takes physical resources, vitualizes them, handles tricky issues related to concurrency, and stores files persistently and safe to access over the long term.</p> <p>Finding the right tradeoffs in these areas is the key to building an operating system. One of the goals in designing and implementing an operating system is to provide high performance and minimize overhead. Overhead comes in many forms - extra time, more instructions, extra space in memory or disk.</p> <p>Isolation is one of the main principles of operating systems. Isolating processes from one another and the operating system in general and is the key to protection.</p> <p>Code that runs on behalf of the OS is special. It has control of devices and should be treated differently than normal application code.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#system-calls","title":"System calls","text":"<p>The idea of system calls is to provide a special pair of hardware instructions and hardware state to make a controlled process. The key differnece between a system call and a procedure call is that a system call transfers control (i.e. jumps) into the kernel space while simlutaneously rasing the hardware privilege level. User applications run in user mode, which means the hardware restricts what applications can do - an application typically cannot initiate an I/O request to disk.  When a system call is initiated a special hardware instruction called a trap.  The hardware transfers control to a pre-specified trap hanlder and raises the privlege level to kernel model. In kernel mode the OS has full access to the hardware and can do things like I/O, network requests, or issue more memory to a program. When the OS is done with the request, it passes control back to the user via return from trap instructions that revert to user mode and pass control back to where the applcation left off.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#multiprogramming","title":"Multiprogramming","text":"<p>Multiprogramming is a way to make better use of machine resources. Insteafd of just running one job at a time, the OS would load multiple jobs into memory and switch rapidly between them, improving CPU utilization. Switching is important because I/O devices are slow and having a program wait on CPU while an I/O request was serviced wasted prescious CPU time.  Memory protection keeps programs from accessing one anothers memory. Concurrency issues while jobs were waiting and resuming in the presence of interrupts led to developments in scheduling.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_4_the_process/","title":"The Process","text":"<p>A process is a running program. A program is a bunch of instructions on disk. The operating system gets these running and transforms the program into something useful.</p> <p>How does the OS provide the illusion of many CPUs?</p> <p>The OS creates the illusion of many CPUs by virtualizing the CPU. By running one process, then quickly stopping and running another, the OS can create an illusion that dozens of CPUs exist when in fact there is one (or a few). This technique is known as time sharing the CPU.</p> <p>Mechanisms are the low-level methods and protocols that implement a piece of functionality. For example, a context switch gives the OS the ability to stop running one program and start running another on a given CPU.</p> <p>On top of mechanisms, policies are the intelligence algorithms for making some kind of decision in the OS. A scheduling policy decides which program to run from a list of potentially runnable programs.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_4_the_process/#the-abstraction-of-a-running-program-a-process","title":"The abstraction of a running program: A process","text":"<p>The abstraction the OS provides of a running program is called a process. To understand a process we have to understand what constitutes its state at any point in time.</p> <p>One important component of state include its machine state, which is made up of memory (i.e. address space), CPU registers and I/O information (i.e. which file are open).  Special CPU registers include the program counter (i.e. instruction pointer), stack pointer and frame pointer are used to manage th stack (points to function parameters, address spaces, and local variables).</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_4_the_process/#the-process-api","title":"The process API","text":"<p>Process APIs include system calls to create and destroy processes and manage them. Examples include <code>fork()</code>, <code>exec()</code>, <code>wait()</code> and <code>exit()</code>. Other management tasks include getting status information and other controls such as suspending and resuming the process.</p> <p>Process creation</p> <ol> <li>The first thing the OS must do is load its code and static data from disk into memory in the address space of the process. Modern OSes perform this task lazily, meaning that the code is loaded only as they are needed during program execution.</li> <li>The OS must allocate memory for the program's stack and heap. The stack is used to manage function calls and local variables, while the heap is used for dynamically allocated memory (e.g., memory allocated with <code>malloc()</code> and freed with <code>free()</code>, data structures like linked lists, hash tables, and trees).</li> <li>Each process has three open file descriptors for standard input, standard output and standard error. The OS must set these up so that the process can read input from the terminal and write output to the screen.</li> <li>Finally the OS will start the program running at the entry point <code>main()</code> and the transfer control of the CPU to the process to begin execution.</li> <li>The OS will also set up the process control block (PCB) which contains all the information about the process, including its state, memory usage, open files, and other resources.</li> </ol>"},{"location":"linux/operating_systems_three_easy_pieces/ch_4_the_process/#process-states","title":"Process states","text":"<p>A process can be in one of several different states at a given time:</p> <ul> <li>Running: a process is running on the processor and executing instructions</li> <li>Ready: A process is ready to run, but the OS is choosing not to run it at this moment.</li> <li>Blocked: A process has performed some kind of operation that makes it not ready until another event takes place. For example, when a process initiates I/O requests to disk, it is blocked and another process can use the processor. Once I/O completes (or some other event causing blocking, like waiting on a network packet) the process is moved to Ready.</li> </ul> <p>Scheduling: Being moved from ready to running means the process has been scheduled. Moved from running to ready means the process has been descheduled. Decisions on which processes to run and when are made by the Scheduler.</p>"},{"location":"linux/operating_systems_three_easy_pieces/ch_4_the_process/#data-structures","title":"Data Structures","text":"<p>The OS has key data structures that track various pieces of process information, such as the state of each process. The OS keeps a process list (aka task list) for all ready processes, blocked processes, When a processes is blocked the contents of the process's registers must be saved to memory. When the process resumes the registers are restored by moving them from memory to the physical registers on the CPU so the process resumes. This is called a context switch.</p> <p>Process control block (PCB): The structure that stores information about a given process (program counter, stack pointer, PID etc)</p> <p></p>"},{"location":"system_design/kafka/","title":"Apache Kafka Overview","text":"<p>kafka design docs</p> <p>What is apache kafka?</p> <ul> <li>Apache Kafka is a distributed streaming platform that is used publish and subscribe to event streams.</li> <li>event streams - capture real-time data from event sources like databases, sensors, mobile devices, cloud services, and software application.</li> <li> <p>capabilities include captureing event streams, storing them durably for as long as needed, processing and reacting to events, and routing the event streams to different destinations.</p> </li> <li> <p>Event stream use cases:</p> </li> <li>payment processing and real time financial transactions, banks, stocks</li> <li>monitoring cars, trucks, fleets and shipments in logistics industry</li> <li>capture and analyze sensor data in real time from IOT devicies, factories</li> <li>Collect and react to customer interactions and orders in retail, hotel, travel</li> <li>Monitor patients in hospital care and predict patient outcomes</li> <li>Foundation for microservices, event driven architecture and real time analytics</li> </ul> <p>How does Kafka work?</p>"},{"location":"system_design/kafka/#-it-is-a-distributed-system-consisting-of-servers-and-clients-that-communicate-via-a-custom-high-performance-tcp-network-protocol","title":"- It is a distributed system consisting of servers and clients that communicate via a custom high-performance TCP network protocol.","text":"<p>Components of Kafka:</p> <p>Servers:</p> <ul> <li>broker - Kafka server that stores data and serves clients</li> <li>zookeeper - Kafka uses Zookeeper to manage the cluster. Zookeeper is a distributed coordination service that Kafka uses to manage and coordinate brokers.</li> <li>controller - Kafka broker that is responsible for managing the state of partitions and replicas and for performing administrative tasks like reassigning partitions and electing leaders.</li> <li>kafka connect - server that imports and exports data as event streams to and from relationa databases and other systems.</li> <li>clients - integrate various languages with kafka</li> </ul> <p>Main Concepts:</p> <ul> <li>Event - an event records the fact something happened. It is also called a record or message. Kafka writes data in the form of events. Events have a key, value, timestamp, and metadata headers. Events are processed exactly once.</li> <li>Producer - applications that write/publish events to Kafka. The producer sends data directly to the broker that is the leader for the partition without any intervening routing tier. To help the producer do this all Kafka nodes can answer a request for metadata about which servers are alive and where the leaders for the partitions of a topic are at any given time to allow the producer to appropriately direct its requests.</li> <li>Consumer - applications that subscribe/read and process events. Producers and consumers are fully decoupled from each other. The Kafka consumer works by issuing \"fetch\" requests to the brokers leading the partitions it wants to consume. The consumer specifies its offset in the log with each request and receives back a chunk of log beginning from that position. The consumer thus has significant control over this position and can rewind it to re-consume data if need be. Every consumer has a consumer group.</li> <li>Broker - a kafka server/container that receives and stores messages from a producer and stores them durably. Brokers save data to disk. data is pushed to the broker from the producer and pulled from the broker by the consumer. One broker serves as the controller. Each broker hosts some set of partitions nd handles incoming requests to write new events to those partitions, read events, and handle replication between each other.</li> <li>Topic - events are stored in topics. Topics are like folders and events like files in a folder. Topics are multi-producer and multi-subscriber. Events in a topic can be read as often as needed. Unlike traditional message queues, events are not deleted after consumption, you define for how long Kafka should retain events via per-topic config settings. Old events are discarded. Storing data is fine, as performance is constant with respect to data size.</li> <li>Partition - Topics are divided into partitions, meaning it is spread over a number of buckets on different kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. To make your data fault-tolerant and highly-available, every topic can be replicated, even across geo-regions or datacenters, so that there are always multiple brokers that have a copy of the data just in case things go wrong, you want to do maintenance on the brokers, and so on. A common production setting is a replication factor of 3, i.e., there will always be three copies of your data. When a new event is published to a topic, it is actually appended to one of the topic's partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written.</li> <li> <p>Offset -  an offset is a unique identifier assigned to each record (message) within a partition of a Kafka topic. the Kafka offset represents the position of a message within that partition\u2019s log. It indicates how far the message is from the beginning of the partition log. Kafka uses offsets to track messages from initial writing to final processing completions.  Ofsets are stored persistently by Kafka, allowing consumers to resume from a specific point in the event of a failure. Every consumer notifies kafka by committing the offset information it has processed. Please note, offset is unique only within a partition, not across partitions. </p> </li> <li> <p>Zookeeper</p> </li> <li> </li> <li>Replication - Kafka replicates the log for each topic's partitions across a configurable number of servers (set by per topic replication factor). This allows automatic failover to replicas if a server in the cluster fails so messages remain available. The unit of replication is the topic partition. Each partition has a single leader and zero or more followers; the total numbmer of replicas including the leader consitute the replication factor.</li> <li>Leaders and followers in replication - In kafka, when replicating a topic's partitions, all writes go to the leader of the partition. Reads go to the leader or follower. There are many more partitions than brokers, and leaders are distributed evenly among brokers. Logs on the leader and follower are identical and have the same offsets and messages, but the leader may have a few unreplicated messages at the end of its log. Followers pull and consume messages from the leader and allows them to batch together log entries. During a failure, only members from the ISR are eligible for election as leader because the ISR (i.e. In sync replicas) is by definition caught up to the leader. A write to a Kafka partition is not considered committed until all in-sync replicas have received the write. This ISR set is persisted in the cluster metadata whenever it changes. Because of this, any replica in the ISR is eligible to be elected leader.</li> </ul>"},{"location":"system_design/kafka/#consumer-group-a-consumer-group-is-a-set-of-consumers-that-consume-from-the-same-topic-kafka-ensures-that-a-partition-is-assigned-to-only-one-consumer-at-a-time-this-helps-kafka-remove-the-complexities-of-sharing-messages-within-a-single-partition-with-multiple-consumers-the-only-remaining-problem-is-ensuring-the-consumers-get-reliable-data-from-the-assigned-partitions-this-is-where-offsets-come-into-the-picture-kafka-uses-offsets-to-track-messages-from-initial-writing-to-final-processing-completions","title":"Consumer Group - A consumer group is a set of consumers that consume from the same topic. Kafka ensures that a partition is assigned to only one consumer at a time. This helps Kafka remove the complexities of sharing messages within a single partition with multiple consumers. The only remaining problem is ensuring the consumers get reliable data from the assigned partitions. This is where offsets come into the picture. Kafka uses offsets to track messages from initial writing to final processing completions.","text":""},{"location":"system_design/kafka/#-controller-a-special-node-in-kafka-that-is-responsible-for-managing-the-registration-of-brokers-in-the-cluster-in-order-to-be-considered-active-a-broker-must-maintain-an-active-session-with-the-controller-to-receieve-meta-data-updates-an-active-session-is-often-maintained-via-heartbeat-checks-and-brokers-acting-as-followers-must-replicate-writes-from-the-leader-and-not-fall-too-far-behind-nodes-satisfying-these-conditions-are-considered-in-sync-and-referred-to-as-the-isr-in-sync-replicas-when-a-follower-dies-or-falls-too-far-behind-ie-replica-lag-time-exceeds-max-configuration-it-is-removed-from-the-isr","title":"- Controller - A special node in kafka that is responsible for managing the registration of brokers in the cluster. In order to be considered \"Active\" - a Broker must maintain an active session with the controller to receieve meta data updates (an active session is often maintained via heartbeat checks) and brokers acting as followers must replicate writes from the leader and not fall too far behind. Nodes satisfying these conditions are considered \"in sync\" and referred to as the ISR (in sync replicas). when a follower dies or falls too far behind (.i.e. replica lag time exceeds max configuration), it is removed from the ISR.","text":"<ul> <li>Messages are considered commited when all replicas in the ISR for that partition have applied it to their log. Only commited messages are given out to the consumer, consumers don't need to worry about missing messages if a leader fails.</li> <li>Producers can wait for messages to be commited or not depending on their trade off for latency and durabilty, as controlled by the acks setting the producer users. Kafka guarantees that a committed message will not be lost, as long as there is at least one in sync replica alive, at all times.</li> </ul> <p>-</p> <p>Problems:</p> <ul> <li>consumer lag - One of Kafka's key performance indicators is consumer lag. It represents the difference between the committed offset and the log-end offset. A minimal lag between log-end offset and committed offset is expected during normal operations. However, if the lag increases, it may break the system. The most common reason for high lag is unpredictable surges in incoming messages, as well as uneven data distribution across partitions, and slow processing jobs.</li> <li>Kafka splits data into partitions by considering the hash of the message key. If you customize the message key and the message volume with a specific key is higher than others, the consumer catering to that partition experiences a high load, leading to high lag.</li> </ul> <p>What if all the replicas and leaders die?</p> <ul> <li>This is a simple tradeoff between availability and consistency. If we wait for replicas in the ISR, then we will remain unavailable as long as those replicas are down. If such replicas were destroyed or their data was lost, then we are permanently down. If, on the other hand, a non-in-sync replica comes back to life and we allow it to become leader, then its log becomes the source of truth even though it is not guaranteed to have every committed message. By default from version 0.11.0.0, Kafka chooses the first strategy and favor waiting for a consistent replica.</li> </ul> <p>Look over log compaction then done: 4.8: https://kafka.apache.org/documentation/#design</p>"}]}