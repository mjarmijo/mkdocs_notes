{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Mike's Site","text":"<p>Storage for my notes on Linux, K8s, and other things.</p>"},{"location":"0-inbox/Class%20methods/","title":"Class methods","text":""},{"location":"0-inbox/Class%20methods/#links","title":"Links","text":"<p>[[python]] [[oop]]</p>"},{"location":"0-inbox/Class%20methods/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/Class%20methods/#summary","title":"Summary","text":"<ul> <li>Class methods work with the entire class, not the individual instance. We may want to update the training level of all superheros at once in the for the superhero instances we created earlier. </li> <li>Class methods are shared by all instances of the class, they do not use the <code>self</code> keyword because they do not have access to the instance attributes. </li> <li>Instead of <code>self</code>, class methods use the <code>cls</code> parameter  for access, and the <code>@classmethod</code> decorator <pre><code>class Superhero:\n    training_level = 1  # Class attribute\n\n    def __init__(self, name: str, power: str):\n        self.name = name         # Instance attribute\n        self.power = power       # Instance attribute\n\n    @classmethod\n    def upgrade_training(cls) -&gt; None:\n        cls.training_level += 1\n        print(f\"All heroes now at training level {cls.training_level}\")\n\nSuperhero.upgrade_training()     # Recommend way to use class method\nprint(Superhero.training_level)  # 2\n</code></pre></li> </ul> <pre><code>class Library:\nbooks_available = 100 # Total books in library\n\n@classmethod\ndef lend_books(cls, number: int) -&gt; None:\ncls.books_available -= number\n\n@classmethod\ndef return_books(cls, number: int) -&gt; None:\ncls.books_available += number\n</code></pre>"},{"location":"0-inbox/Class%20methods/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Class%20methods/#tags","title":"Tags","text":"<p>2026-02-23-1700</p>"},{"location":"0-inbox/Getter%20and%20setter%20methods/","title":"Getter and setter methods","text":""},{"location":"0-inbox/Getter%20and%20setter%20methods/#links","title":"Links","text":"<p>OOP Getter and setter methods</p>"},{"location":"0-inbox/Getter%20and%20setter%20methods/#3-key-points","title":"3 Key Points","text":"<ul> <li>getter and setters often include validation to make sure private/protected methods are not set to invalid values</li> <li>names always begin with get/set_attribute</li> </ul>"},{"location":"0-inbox/Getter%20and%20setter%20methods/#summary","title":"Summary","text":"<ul> <li> <p>Methods that get/set a value of a private/protected method.  <pre><code>class SuperHero:\n    def __init__(self, name: str, power_level: int):\n        self.__name = name                # private attribute\n        self.__power_level = power_level  # private attribute\n\n    # Getter method to get power_level\n    def get_power_level(self) -&gt; int:\n        return self.__power_level\n\n    def set_power_level(self, new_level: int) -&gt; None:  # Setter method to set power_level\n        if 0 &lt;= new_level &lt;= 100:          # validation\n            self.__power_level = new_level\n        else:\n            print(\"Power level must be between 0 and 100!\")\n</code></pre></p> </li> <li> <p>Includes validation to make sure we never set a private method to an invalid value. We could also raise an error like <code>raise ValueError(\"Power level must be between 0 and 100)</code> instead of printing an error</p> </li> <li>always use the name get_(attribute) and set(attribute) for getter and setter methods</li> </ul>"},{"location":"0-inbox/Getter%20and%20setter%20methods/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Getter%20and%20setter%20methods/#tags","title":"Tags","text":"<p>2026-02-21-0651</p>"},{"location":"0-inbox/Method%20overriding/","title":"Method overriding","text":""},{"location":"0-inbox/Method%20overriding/#links","title":"Links","text":"<p>[[python]] [[OOP]]</p>"},{"location":"0-inbox/Method%20overriding/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/Method%20overriding/#summary","title":"Summary","text":"<ul> <li>Method overriding allows you to change the behavior of an inherited function in the child class <pre><code>class Superhero:\n    def __init__(self, name: str):\n        self.name = name\n\n    def fight(self) -&gt; None:\n        print(\"Superhero fights with advanced weapons!\")\n\nclass Avenger(Superhero):\n    # Override the fight method\n    def fight(self) -&gt; None:\n        print(\"Avenger fights with advanced weapons!\")\n</code></pre></li> </ul>"},{"location":"0-inbox/Method%20overriding/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Method%20overriding/#tags","title":"Tags","text":"<p>2026-02-27-0855</p>"},{"location":"0-inbox/Multiple%20Inheritance%20-%20Diamond%20Problem/","title":"Multiple Inheritance   Diamond Problem","text":""},{"location":"0-inbox/Multiple%20Inheritance%20-%20Diamond%20Problem/#links","title":"Links","text":"<p>[[python]] [[OOP]] Multiple Inheritance</p>"},{"location":"0-inbox/Multiple%20Inheritance%20-%20Diamond%20Problem/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/Multiple%20Inheritance%20-%20Diamond%20Problem/#summary","title":"Summary","text":"<pre><code>class A:\n    def print_method(self) -&gt; None:\n        print(\"A\")\n\nclass B(A):\n    def print_method(self) -&gt; None:\n        print(\"B\")\n\nclass C(A):\n    def print_method(self) -&gt; None:\n        print(\"C\")\n\nclass D(B, C):\n    pass\n\nd = D()\nd.print_method()  # Which method will be called? (B)\n</code></pre> <p>This inheritance structure creates a diamond shape, as illustrated below:</p> <pre><code>   A\n  / \\\n B   C\n  \\ /\n   D\n</code></pre> <p>Python uses Method Resolution Order (MRO) to determine which method to call when dealing with multiple inheritance. Here's how it works:</p> <ul> <li>First, Python looks for the method in the current class <code>D</code></li> <li>If not found, it checks the first parent class <code>B</code> as it is the first parent class passed to the <code>D</code> class</li> <li>Then the second parent class <code>C</code> as it is the second parent class passed to the <code>D</code> class</li> <li>Finally, it checks the base class <code>A</code></li> </ul> <p>You can view this order using the <code>__mro__</code> attribute:</p> <pre><code>print(D.__mro__)\n# Output: [&lt;class '__main__.D'&gt;, &lt;class '__main__.B'&gt;, &lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class 'object'&gt;\n</code></pre> <p>The MRO is <code>[D, B, C, A]</code>.</p>"},{"location":"0-inbox/Multiple%20Inheritance%20-%20Diamond%20Problem/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Multiple%20Inheritance%20-%20Diamond%20Problem/#tags","title":"Tags","text":"<p>2026-02-28-0656</p>"},{"location":"0-inbox/Multiple%20Inheritance/","title":"Multiple Inheritance","text":""},{"location":"0-inbox/Multiple%20Inheritance/#links","title":"Links","text":"<p>[[python]] [[OOP]]</p>"},{"location":"0-inbox/Multiple%20Inheritance/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/Multiple%20Inheritance/#summary","title":"Summary","text":"<ul> <li>A child class can also inherit methods and attributes from more than one parent, hence multiple inheritance</li> <li>This is generally considered an antipattern <pre><code>class Swimmer:\n    def swim(self):\n        print(\"Swimming\")\n\nclass Flyer:\n    def fly(self):\n        print(\"Flying\")\n\n# Ducks inherits from both Swimmer and Flyer\nclass Duck(Swimmer, Flyer):\n    pass\n\nduck = Duck()\nduck.swim() # Should print \"Swimming\"\nduck.fly()  # Should print \"Flying\"\n</code></pre></li> <li>Use <code>pass</code> to indicate we are not adding any new methods to Duck. It will automatically have the fly and swim methods</li> <li>Note that python's method resolution order (MRO) defaults to the first parent. So if one parent has a different <code>__init__</code> method, it may be ignored or modified.  <pre><code>class ElectronicDevice:\n    def __init__(self, brand: str, model: str):\n        self.brand = brand\n        self.model = model\n    def turn_on(self) -&gt; None:\n        print(\"Device is turning on\")\n    def turn_off(self) -&gt; None:\n        print(\"Device is turning off\")\n\nclass HealthDevice:\n    def __init__(self, brand: str, model: str, cost: int):\n        self.brand = brand\n        self.model = model\n        self.cost = cost\n    def measure_heart_rate(self) -&gt; None:\n        print(\"Measuring heart rate\")\n\n## This works but SmartWatch(ElectronicDevice, HealthDevice) would not inherit \"cost\" attribute  \nclass SmartWatch(HealthDevice, ElectronicDevice):\n    pass\n\n# Do not modify the code below\nsmart_watch = SmartWatch(\"Apple\", \"Watch Series 6\")\nsmart_watch.turn_on()\nsmart_watch.measure_heart_rate()\nsmart_watch.turn_off()\nprint(smart_watch.cost)\n</code></pre></li> <li>Most of the time multiple inheritance should not be used because it can lead to complexity and conflicts, but you may see it in a code base.  </li> </ul>"},{"location":"0-inbox/Multiple%20Inheritance/#further-reading","title":"Further Reading","text":"<pre><code>class Hero:\n    def __init__(self, name: str, power_level: int, health: int):\n        self.name = name\n        self.power_level = power_level\n        self.health = health\n\n    def use_power(self) -&gt; str:\n        return f\"{self.name} uses their power!\"\n\n# TODO: Implement the FlightHero and StrengthHero classes\nclass FlightHero(Hero):\n    def __init__(self, name: str, power_level: int, health: int, flight_speed: int):\n        super().__init__(name, power_level, health)\n        self.flight_speed = flight_speed\n\n    def use_power(self) -&gt; str:\n        return f\"{self.name} flies at {self.flight_speed} mph!\"\n\nclass StrengthHero(Hero):\n    def __init__(self, name: str, power_level: int, health: int, lifting_capacity: int):\n        super().__init__(name, power_level, health)\n        self.lifting_capacity = lifting_capacity\n\n    def use_power(self) -&gt; str:\n        return f\"{self.name} lifts {self.lifting_capacity} pounds!\"\n\n# Don't change the code below\nflight_hero = FlightHero(\"Superman\", 10, 100, 1000)\nstrength_hero = StrengthHero(\"Hulk\", 10, 100, 1000)\nprint(flight_hero.name)\nprint(flight_hero.power_level)\nprint(flight_hero.health)\nprint(flight_hero.flight_speed)\nprint(flight_hero.use_power())\nprint(strength_hero.name)\nprint(strength_hero.power_level)\nprint(strength_hero.health)\nprint(strength_hero.lifting_capacity)\nprint(strength_hero.use_power())\n</code></pre>"},{"location":"0-inbox/Multiple%20Inheritance/#tags","title":"Tags","text":"<p>2026-02-28-0639</p>"},{"location":"0-inbox/OOP/","title":"OOP","text":""},{"location":"0-inbox/OOP/#links","title":"Links","text":"<p>Public attribute and methods Protected attributes and methods encapsulation [[Private attributes and methods]] [[Getter and setter methods]] [[Property and setter decorators]] [[class attributes]] Class methods [[static methods]] [[inheritance]] Method overriding Super function Multiple Inheritance Multiple Inheritance - Diamond Problem</p>"},{"location":"0-inbox/OOP/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/OOP/#summary","title":"Summary","text":""},{"location":"0-inbox/OOP/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/OOP/#tags","title":"Tags","text":"<p>2026-02-20-0653</p>"},{"location":"0-inbox/Private%20attributes%20and%20methods/","title":"Private attributes and methods","text":""},{"location":"0-inbox/Private%20attributes%20and%20methods/#links","title":"Links","text":"<p>Protected attributes and methods [[python]] [[oop]]</p>"},{"location":"0-inbox/Private%20attributes%20and%20methods/#3-key-points","title":"3 Key Points","text":"<p>Private methods: -  should not be directly accessed using normal dot notation from outside the class. - are meant to be used only within the defining class itself. -  provides the strongest form of information hiding in Python. Use for sensitive info. But remember private attributes can still be accessed!! </p>"},{"location":"0-inbox/Private%20attributes%20and%20methods/#summary","title":"Summary","text":"<ul> <li>Private attributes and methods are prefixed with a double underscore <code>__</code> and should not be accessed from outside the class.  Use these for sensitive info. </li> <li>Private attributes are not accessible from child classes, but protected attribute are accessible from children <pre><code>class SuperHero:\n    def __init__(self, name: str, power_level: int):\n        self.__name = name                # private attribute\n        self.__power_level = power_level  # private attribute\n\n    # private method\n    def __secret_power(self) -&gt; str:\n        return f\"Using {self.__name}'s secret power!\"\n\n    # public method to access private method\n    def use_power(self) -&gt; str:\n        return self.__secret_power()\n\n    # public method to access private attribute\n    def get_power_level(self) -&gt; int:\n        return self.__power_level\n</code></pre></li> </ul>"},{"location":"0-inbox/Private%20attributes%20and%20methods/#-use-public-methods-to-access-private-methods-and-attributes","title":"- Use public methods to access private methods and attributes","text":""},{"location":"0-inbox/Private%20attributes%20and%20methods/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Private%20attributes%20and%20methods/#tags","title":"Tags","text":"<p>2026-02-21-0638</p>"},{"location":"0-inbox/Property%20and%20setter%20decorators/","title":"Property and setter decorators","text":""},{"location":"0-inbox/Property%20and%20setter%20decorators/#links","title":"Links","text":"<p>OOP Getter and setter methods</p>"},{"location":"0-inbox/Property%20and%20setter%20decorators/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/Property%20and%20setter%20decorators/#summary","title":"Summary","text":"<ul> <li> <p>Use decorators <code>@property</code> and <code>@setter</code> for the most pythonic way of using getters and setters.  <pre><code>class Hero:\n    def __init__(self, name: str):\n        self.__name = name    # private attribute\n\n    # Getter\n    @property\n    def name(self) -&gt; str:\n        return self.__name\n\n    # Setter\n    @name.setter\n    def name(self, new_name: str) -&gt; None:\n        if new_name != \"\":\n            self.__name = new_name\n        else:\n            print(\"Name cannot be empty!\")\n</code></pre></p> </li> <li> <p>The property <code>__name</code> is still private, but the getter and setter method are not</p> </li> <li>Use the decorates because it makes code cleaner/less verbose. </li> </ul> <pre><code>class SuperHero:\n\ndef __init__(self, name: str, health: int, power_level: int):\n    self.name = name\n    self.__health = health\n    self.__power_level = power_level\n\n@property\ndef health(self) -&gt; int:\n    return self.__health\n\n@property\ndef power_level(self) -&gt; int:\n    return self.__power_level\n\n@health.setter\ndef health(self, health: int) -&gt; None:\n    if health &lt; 0:\n    print(\"You can't set the health to less than 0\")\n    elif health &gt; 100:\n    print(\"You can't set the health to more than 100\")\n    else:\n    self.__health = health\n\n@power_level.setter\ndef power_level(self, power_level: int) -&gt; None:\n    if power_level &lt; 1:\n    print(\"You can't set the power level to less than 1\")\n    elif power_level &gt; 10:\n    print(\"You can't set the power level to more than 10\")\n    else:\n    self.__power_level = power_level\n\n\nsuper_hero = SuperHero(\"Batman\", 80, 9)\nprint(super_hero.health) # this should print 80\nsuper_hero.health = 110 # this should print You can't set the health to more than 100\nprint(super_hero.power_level) # this should print 9\nsuper_hero.power_level = 100 # this should print You can't set the power level to more than 10\nsuper_hero.power_level = 0 # this should print You can't set the power level to less than 1\n\nprint(f\"{super_hero.name} has {super_hero.health} health and {super_hero.power_level} power level\")\n</code></pre>"},{"location":"0-inbox/Property%20and%20setter%20decorators/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Property%20and%20setter%20decorators/#tags","title":"Tags","text":"<p>2026-02-21-0705</p>"},{"location":"0-inbox/Protected%20attributes%20and%20methods/","title":"Protected attributes and methods","text":""},{"location":"0-inbox/Protected%20attributes%20and%20methods/#links","title":"Links","text":"<p>Private attributes and methods python oop</p>"},{"location":"0-inbox/Protected%20attributes%20and%20methods/#3-key-points","title":"3 Key Points","text":"<ul> <li>Use public methods/attributes to access protected methods/attributes</li> <li>Protected (single underscore <code>_</code> ): Should only be accessed within class and subclasses</li> </ul>"},{"location":"0-inbox/Protected%20attributes%20and%20methods/#summary","title":"Summary","text":"<ul> <li>Protected attributes and methods should not be access from outside the class, but they can be access inside the class and it's children. They are still available outside the class but they should not be accessed directly.</li> <li> <p>Define a protected attribute by prefixing the name with a single <code>_</code>. This is not enforced by the interpreter - it is a convention to warn developers not to use it directly.  <pre><code>class SuperHero:\n    def __init__(self, name: str, power_level: int):\n        self._name = name                # protected attribute\n        self._power_level = power_level  # protected attribute\n\n    def get_name(self) -&gt; str: # public method\n        return self._name\n\n    def _some_protected_method(self) -&gt; None: # protected method\n        pass\n\n    def some_public_method(self) -&gt; None:\n        self._some_protected_method()\n</code></pre></p> </li> <li> <p>Access private attributes only via public methods <pre><code>spider_man = SuperHero(\"Spider-Man\", 85)\n\nprint(spider_man._name)      # Allowed but discouraged\nprint(spider_man.get_name()) # Recommended\n\nspider_man._some_protected_method() # Allowed but discouraged\nspider_man.some_public_method()     # Recommended\n</code></pre></p> </li> </ul>"},{"location":"0-inbox/Protected%20attributes%20and%20methods/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Protected%20attributes%20and%20methods/#tags","title":"Tags","text":"<p>2026-02-20-0653</p>"},{"location":"0-inbox/Public%20attribute%20and%20methods/","title":"Public attribute and methods","text":""},{"location":"0-inbox/Public%20attribute%20and%20methods/#tags","title":"Tags","text":"<p>[[python]] [[oop]] Private attributes and methods Public attribute and methods</p>"},{"location":"0-inbox/Public%20attribute%20and%20methods/#3-key-points","title":"3 Key Points","text":"<ul> <li>Public attributes are accessible everywhere</li> </ul>"},{"location":"0-inbox/Public%20attribute%20and%20methods/#summary","title":"Summary","text":"<ul> <li>public attributes and methods can be used or modified from outside the class, by default all are public</li> </ul> <pre><code>class SuperHero:\n    def __init__(self, name: str, power_level: int):\n        self.name = name                # public attribute\n        self.power_level = power_level  # public attribute\n\n    # Public method\n    def display_power_level(self) -&gt; None:\n        print(f\"{self.name} has a power level of {self.power_level}\")\n</code></pre> <ul> <li>When a method or attribute is public we can access it via dot notation, modify it or call it from outside the class</li> </ul> <pre><code># Creating a superhero\nspider_man = SuperHero(\"Spider-Man\", 85)\n\n# Accessing public attributes/methods\nprint(spider_man.name)\nspider_man.display_power_level()\n\n# Modifying public attributes\nspider_man.power_level = 90\n</code></pre>"},{"location":"0-inbox/Public%20attribute%20and%20methods/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Public%20attribute%20and%20methods/#links","title":"Links:","text":"<p>2026-02-20-0639</p>"},{"location":"0-inbox/SSH%20Authentication%20to%20a%20VM/","title":"SSH Authentication to a VM","text":""},{"location":"0-inbox/SSH%20Authentication%20to%20a%20VM/#links","title":"Links","text":"<p>Linux [[kubecraft]]</p>"},{"location":"0-inbox/SSH%20Authentication%20to%20a%20VM/#3-key-points","title":"3 Key Points","text":"<ul> <li>How to set up a VM for testing/learning</li> </ul>"},{"location":"0-inbox/SSH%20Authentication%20to%20a%20VM/#summary","title":"Summary","text":"<ul> <li>Use Linux virtual machine manager</li> <li>Install ubuntu 24.04 iso</li> <li>Enable ssh, leave everything else stock</li> <li> <p>Install details: User: $Username password: $XXXX</p> </li> <li> <p>How to force password auth</p> </li> </ul> <pre><code>ssh -o PreferredAuthentications=password user@192.168.122.40\n</code></pre> <p>ssh config: <pre><code>Host wakka\n    HostName 192.168.122.40\n    User user\n    PubkeyAuthentication no\n    PasswordAuthentication yes\n    PreferredAuthentications password,keyboard-interactive\n</code></pre></p>"},{"location":"0-inbox/SSH%20Authentication%20to%20a%20VM/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/SSH%20Authentication%20to%20a%20VM/#tags","title":"Tags","text":"<p>2026-02-24-0740</p>"},{"location":"0-inbox/Super%20function/","title":"Super function","text":""},{"location":"0-inbox/Super%20function/#links","title":"Links","text":"<p>[[python]] [[OOP]]</p>"},{"location":"0-inbox/Super%20function/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/Super%20function/#summary","title":"Summary","text":"<ul> <li>Super is a built-in function that allows you to  call methods from the parent class</li> <li>Sometimes we need to extend, and not replace the parent behavior such as adding extra steps to the initialization <code>__init__</code> method, access parent  class properties, or add functionality to a method.</li> <li><code>Super()</code>doesn't take any arguments, it is just used to call parent class properties and methods. </li> </ul> <pre><code>class ParentClass:\n    def parent_method(self) -&gt; None:\n        print(\"This is the parent class method\")\n\nclass ChildClass(ParentClass):\n    def child_method(self) -&gt; None:\n        super().parent_method()\n        print(\"This is the child class method\")\n\n## Syntax\nsuper().parent_method() # Call parent class's instance method\n\nsuper().__init__() # Call parent class __init__ method\n\nsuper().parent_property # Access parent class property\n</code></pre> <pre><code>class SuperHero:\n    def __init__(self, name: str, power: str):\n        self.name = name\n        self.power = power\n\n    def attack(self) -&gt; None:\n        print(f\"{self.name} is attacking with {self.power}\")\n\nclass Avenger(SuperHero):\n    def __init__(self, name: str, power: str, team: str):\n        super().__init__(name, power)\n        self.team = team\n\n# Don't change the code below\navenger = Avenger(\"Iron Man\", \"repulsor beams\", \"Avengers\")\nprint(avenger.name)\nprint(avenger.power)\nprint(avenger.team)\navenger.attack()\n</code></pre>"},{"location":"0-inbox/Super%20function/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/Super%20function/#tags","title":"Tags","text":"<p>2026-02-27-0901</p>"},{"location":"0-inbox/ZettleKasten%20method%20overview/","title":"ZettleKasten method overview","text":"<p>[[ZettleKasten]]</p>"},{"location":"0-inbox/ZettleKasten%20method%20overview/#3-key-points","title":"3 Key Points","text":"<ul> <li>A tool for structured note taking that improves thinking, makes writing easier - it is a thinking tool and not a note taking tool.</li> <li>writing is done while you research and learn, the focus isn't on copying what someone else says but writing down your understanding</li> <li>Developing your system makes you more productive over the long term and improves comprehension</li> </ul>"},{"location":"0-inbox/ZettleKasten%20method%20overview/#summary","title":"Summary","text":"<ul> <li>Note taking system, note cards</li> <li>each cards contains a single idea </li> <li>create links between cards and form a web of knowledge</li> <li>don't copy text, put content into your own words</li> <li>main writing is done during research, organizes your thoughts</li> <li>a thinking tool, not a note taking tool</li> <li>writing ideas in your own words improves understanding</li> <li>Initially, the set up is a lot of work, but it improves productivity over time</li> </ul>"},{"location":"0-inbox/ZettleKasten%20method%20overview/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/ZettleKasten%20method%20overview/#links","title":"Links:","text":"<p>2026-02-17-0941</p>"},{"location":"0-inbox/class%20attributes/","title":"Class attributes","text":""},{"location":"0-inbox/class%20attributes/#links","title":"Links","text":"<p>[[python]] [[oop]]</p>"},{"location":"0-inbox/class%20attributes/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/class%20attributes/#summary","title":"Summary","text":"<ul> <li>class attributes share info across all instances of a class</li> <li>Instance attributes a specific to that particular instance of a class</li> <li>We don't need <code>self</code> to access class attributes. They should be declared within the class, but not as part of init or other methods</li> <li>Use the Class name, not instance name to access the class attributes, i.e. <code>Superhero.hero_count</code></li> <li>You can update them outside of the class, too. </li> </ul> <pre><code>class SmartDevice:\n# Add your class attributes here\ntotal_devices = 0\nactive_devices = 0\n\ndef __init__(self, name: str):\nself.name = name\nSmartDevice.total_devices += 1 # Each new device adds to total\n# Implement these methods\ndef turn_on(self) -&gt; None:\nSmartDevice.active_devices += 1\n\ndef turn_off(self) -&gt; None:\nSmartDevice.active_devices -= 1\n</code></pre>"},{"location":"0-inbox/class%20attributes/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/class%20attributes/#tags","title":"Tags","text":"<p>2026-02-22-1553</p>"},{"location":"0-inbox/encapsulation/","title":"Encapsulation","text":""},{"location":"0-inbox/encapsulation/#links","title":"Links","text":"<p>[[python]] [[oop]]</p>"},{"location":"0-inbox/encapsulation/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/encapsulation/#summary","title":"Summary","text":"<p>Encapsulation - The process of wrapping methods and attributes (i.e. data and functions) in a unit (i.e. class). Access is allowed only through special syntax. Encapsulation hides details of the object and exposes a well defined set of functionality to the user. Example: A car encapsulates a lot of complex components (engine, battery, drive-train) that you don't need to understand in order to drive. </p>"},{"location":"0-inbox/encapsulation/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/encapsulation/#tags","title":"Tags","text":"<p>2026-02-21-0636</p>"},{"location":"0-inbox/inheritance/","title":"Inheritance","text":""},{"location":"0-inbox/inheritance/#links","title":"Links","text":"<p>[[python]] [[OOP]]</p>"},{"location":"0-inbox/inheritance/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/inheritance/#summary","title":"Summary","text":"<ul> <li>Inheritance allows you to create one class from another</li> <li>The new class is known as the child class, the original is the parent class/super class</li> <li>The child inherits properties and methods from the parent/super. It enables code reuse and allows us to expand an existing class.</li> </ul> <pre><code>class Superhero:\n    def __init__(self, name: str, power: str):\n        self.name = name\n        self.power = power\n\nclass Avenger(Superhero):\n    def fly(self) -&gt; None:\n        print(f\"{self.name} can fly using {self.power}\")\n\niron_man = Avenger(\"Iron Man\", \"repulsor beams\")\niron_man.fly() # Iron Man can fly using repulsor beams\n</code></pre> <ul> <li>Avenger inherits the superhero name and power attributes, and expands them with a fly method. </li> <li>The constructor <code>__init__</code> of the parent class is called when an instance of the child class is created. </li> <li></li> </ul>"},{"location":"0-inbox/inheritance/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/inheritance/#tags","title":"Tags","text":"<p>2026-02-26-2136</p>"},{"location":"0-inbox/static%20methods/","title":"Static methods","text":""},{"location":"0-inbox/static%20methods/#links","title":"Links","text":"<p>[[python]] [[OOP]]</p>"},{"location":"0-inbox/static%20methods/#3-key-points","title":"3 Key Points","text":""},{"location":"0-inbox/static%20methods/#summary","title":"Summary","text":"<ul> <li>Similar to class methods, they are regular functions that live inside a class for organizational purposes</li> <li>They do not have access to self, cls, or instance attributes, but they can access class attributes <pre><code>class CurrencyConverter:\n    rates = {  \n        'EUR': 1.20,  # 1 EUR = 1.20 USD\n        'JPY': 0.01   # 1 JPY = 0.01 USD\n    } # Class attribute\n\n    @staticmethod\n    def to_usd(amount: float, code: str) -&gt; float:\n        if code not in CurrencyConverter.rates:\n            raise ValueError(f\"Invalid currency code: {code}\")\n        return amount * CurrencyConverter.rates[code]\n\n\nprint(f\"100 EUR = {CurrencyConverter.to_usd(100, 'EUR')} USD\")     # 120 USD\nprint(f\"100 JPY = {CurrencyConverter.to_usd(100, 'JPY')} USD\")     # 1 USD\n</code></pre></li> </ul>"},{"location":"0-inbox/static%20methods/#further-reading","title":"Further Reading","text":""},{"location":"0-inbox/static%20methods/#tags","title":"Tags","text":"<p>2026-02-23-1859</p>"},{"location":"0-inbox/todo_list/","title":"TO DO","text":"<ul> <li>cards game - make interactive game for users, write out sequence diagram for initializing a game and test</li> <li>k8s - expose mkdocs_notes on a local endpoint</li> <li>k8s - write notes on manifests, control plane, network</li> <li>linux - write notes on IPC, context switching, memory/virtual memory</li> <li>Finish Ruby book on OOP</li> <li></li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/","title":"Claude Intro","text":"<p>[[mcp]] [[claude]] ai agents</p>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#claude-models","title":"Claude models","text":"<p>opus 4.5 - use for complex tasks sonnet 4.5 - use for everyday tasks. Has an extended thinking mode for deeper reasoning. Works through problems step by step making it suited for tasks using careful analysis.</p> <p>What project should use extended thinking? Project planning, planning/trend analysis, technical problems, math, coding. Not necessary for simple questions, basic info, and general writing. </p> <p>Learning mode - guides your reasoning process instead of providing answers to help you develop critical thinking skills</p>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#accessing-claude","title":"Accessing Claude","text":"<p>Claude.ai - mobile and desktop apps - idea for conversations, writing assistance, research, analysis. </p> <p>Claude code - agentic coding tool designed for developers, but can be used for all kinds of file manipulation. It can directly edit files, run commands, create commits.  See here for more details: https://anthropic.skilljar.com/Claude-code-in-action</p> <p>Claude and slack - chat with Claude in the AI assistance header from any channel or conversation, or mention @Claude in threads. It can search your channels, mesages, and files, to find context. </p> <p>Claude for excel - work directly with Claude in excel where it can read, analyze, modify and create new workbooks. Think model analysis, assumption updates, errors/debugging, make templates, explain formulas. </p>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#conversations-with-claude","title":"Conversations with Claude","text":"<ul> <li>talk to it like you would a coworker</li> <li>Stage setting - Set the role, objectives, and context</li> <li>define your task - what are the actions you want it to take?</li> <li>specify rules for style, tone and examples you can attach to show</li> <li>Upload documents or background information so Claude can consider it in response - it's a shortcut to help Claude understand your needs<ul> <li>Uploading documents are a shortcut to helping Claude understand your needs</li> <li>pdf, csv, docs, txt, PNG, JPEG, web search, data sources like drive, calendar etc. and Claude will parse the content</li> </ul> </li> <li></li> <li>organize your conversations into projects with persistent context</li> <li>Artifacts turn your ideas into shareable objects</li> <li>Improve the responses you are getting by asking follow up questions, providing feedback (Can you expand on the second point?\" or \"That's helpful, but can you make it more concise?\") about what is good/bad (\"This is good, but the tone is too formal. Can you make it more conversational?\"), and redirecting it/restarting your conversation. </li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#getting-the-best-results","title":"Getting the best results","text":"<p>4D's of AI Fluency -  - Delegation: Deciding on what work should be done by humans, what work should be done by AI, and how to distribute tasks between them. Includes understanding your goals, AI capabilities, and making strategic choices about collaboration. - Description: Effectively communicating with AI systems. Includes clearly defining outputs, guiding AI processes, and specifying desired AI behaviors and interactions. - Discernment: Thoughtfully and critically evaluating AI outputs, processes, behaviors and interactions. Includes assessing quality, accuracy, appropriateness, and determining areas for improvement. - Diligence: Using AI responsibly and ethically. Includes making thoughtful choices about AI systems and interactions, maintaining transparency, and taking accountability for AI-assisted work. </p> <p>Effective Claude users:</p> <ul> <li>Treat first drafts as starting points. Review what Claude produces, identify what's working and what isn't, then refine.</li> <li>Give specific feedback. \"Make it shorter\" is fine, but \"Cut the first two paragraphs and make the conclusion more action-oriented\" is better.</li> <li>Know when to start fresh. If a conversation has gone off track, sometimes it's faster to open a new chat with a clearer prompt than to try to redirect.</li> </ul> <p>Evals - Evals are systematic ways to test how well Claude performs on specific types of tasks that matter to you. Running simple evals helps you:</p> <ul> <li>Understand where Claude adds the most value in your workflow</li> <li>Identify tasks where you'll need to provide more context or examples</li> <li>Build confidence in Claude's outputs for recurring tasks</li> </ul> <p>A practical Eval approach:</p> <ol> <li>Gather examples. Collect 5-10 examples of a task you do regularly\u2014emails you've written, reports you've created, analyses you've done.</li> <li>Create test prompts. Write prompts that would generate similar outputs. Include the context you'd naturally have when doing this work.</li> <li>Compare outputs. Run your prompts and compare Claude's responses to your examples. Ask yourself:<ul> <li>Does Claude capture the key information?</li> <li>Is the tone and style appropriate?</li> <li>What's missing or could be improved?</li> </ul> </li> <li>Refine your approach. Based on what you learn, adjust your prompts, add examples to show Claude what good looks like, or identify where human review is essential.</li> </ol> <p>To evaluate how Claude might work with your data reproduce a past analysis you've done, then evaluate the quality of its output against known results:</p> <ul> <li>Find a dataset you've manually analyzed</li> <li>Create prompts that request Claude to do the analysis on your behalf</li> <li>Compare Claude's results to your originals</li> <li>Note patterns and refine your prompt accordingly: Maybe Claude gets the right numbers but misses the overall patterns </li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#projects","title":"Projects","text":"<ul> <li>Projects are self-contained workspaces with their own memory, chat histories, knowledge bases, and customized instructions. Think of them as dedicated environments for specific work streams. Each conversation within the project automatically has access to your knowledge base and follows your project instructions.</li> <li>Project knowledge enhances Claude's understanding by letting you upload relevant documents that Claude references across all chats within that project. No more re-uploading the same files each time.</li> <li>Project instructions guide Claude's behavior\u2014you can specify tone, expertise level, response style, and more. These instructions apply to every conversation within the project.</li> <li>Projects scale automatically. When your knowledge base approaches context limits, Claude seamlessly enables Retrieval Augmented Generation (RAG) mode to expand capacity by up to 10x while maintaining response quality.</li> <li>For Claude for Work users, projects enable collaboration. Share projects with teammates so everyone benefits from the same context, instructions, and accumulated knowledge.</li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#instructions","title":"Instructions","text":"<p>Project instructions tell Claude how to behave across all conversations in this project. Click on \"Instructions\" to open the instructions panel.</p> <p>Good project instructions typically include: - Context about what you're working on: \"This project is for creating marketing content for our B2B software product.\" - Process instructions: \"First consider a blog structure that will entice this audience, then write the draft.\" - Tone and style preferences: \"Use a professional but conversational tone. Avoid jargon when possible.\" - Specific requirements: \"Always include a call-to-action at the end of marketing copy.\"</p>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#knowledge-base","title":"Knowledge base","text":"<p>Your project's knowledge base is where you upload documents that Claude should reference. You'll find the files menu on the right side of your project's main page. Click the \"+\" button to add content. You can upload various file types including PDF, DOCX, CSV, TXT, HTML, and more. You can also connect to Google Drive to link documents directly.</p> <p>What to upload: - Reference documents (brand guidelines, style guides, templates) - Background materials (research reports, meeting notes, requirements docs) - Examples of work you want Claude to emulate - Technical documentation or specifications - Pro tip: Name your files descriptively. Claude uses file names to understand and retrieve the right information, so \"Q4-2024-Brand-Guidelines.pdf\" is more helpful than \"document1.pdf.\"</p> <p>Projects automatically scale to handle large amounts through a feature called Retrieval Augmented Generation (RAG). When your project knowledge approaches the context window limit, Claude seamlessly enables RAG mode. Instead of loading all project content into memory at once, Claude intelligently searches and retrieves only the most relevant information needed to answer your questions. This expands your project's capacity by up to 10x while maintaining response quality. You'll see a visual indicator when your project is RAG-enabled, but the experience should feel the same\u2014you can still upload documents, chat with Claude, and get context-aware responses.</p> <p>How a RAG Model Works (The Three Steps) 1. Retrieval: When a user asks a question, the system searches an external data source (like a database, document store, or internet search) for relevant information - Augmentation: The retrieved, relevant data is added to the user's original prompt to provide context. - Generation: The LLM receives the enriched prompt and generates a response based on both its training and the provided information.</p> <p>Key Benefits of RAG - Improved Accuracy: Reduces \"hallucinations\" (confident, false answers) by providing factual,, up-to-date context. - Access to Proprietary Data: Allows AI to answer questions based on private company documents, emails, or databases without retraining the LLM. - Cost-Effective: It is cheaper to use RAG than to fine-tune or retrain large models to learn new information. - Trusted Sources: Provides citations for the information used, allowing users to verify the answer.</p>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#best-practices-for-projects","title":"Best practices for projects","text":"<ul> <li>Start focused, then expand. Begin with a specific use case rather than trying to create one project for everything. You can always add more content as you go.</li> <li>Keep your knowledge base current. Outdated documents can lead to outdated responses. Review and update your project knowledge periodically.</li> <li>Write clear instructions. Be specific about what you want. Vague instructions lead to inconsistent results.</li> <li>Group related documents. This helps Claude draw connections between different sources and provide more comprehensive responses.</li> <li>Reference documents by name. When asking questions, you can mention specific documents to help Claude focus its search: \"Based on our Q3 report, what were the top customer concerns?\"</li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#skills","title":"Skills","text":"<ul> <li>Skills allow Claude to perform specialized, repeatable tasks - they are instructions, scripts and resources that are loaded automatically when needed. </li> <li>Project store knowledge and context, Skills perform tasks.</li> <li>The two features complement each other. A skill can reference knowledge stored in a project\u2014your \"customer call prep\" skill might pull from customer profiles uploaded to a project's knowledge base. The project provides the what (information), the skill provides the how (process).</li> <li>Skills to create excel, PowerPoint, word doc, PDFs etc. </li> <li>Two types: Anthropic and custom<ul> <li>Anthropic are available to paid users and created/maintained by Anthropic --&gt; document creation capabilities</li> <li>Custom - you create for workflows or domain specific tasks. e.g. structure notes in a format, data analysis workflow, or apply guidelines to documents. </li> <li>Skills are steps and order of operations you want followed every time</li> <li>Enable skills by first enabling <code>Code execution and file creation</code></li> <li>settings --&gt; capabilities --&gt; code execution and file creation --&gt; scroll to skills and turn them off/on as needed. You also need to <code>allow limited network access</code> for Claude to edit any files that you upload. </li> <li>Custom skills will be listed in your settings, too<ul> <li>Create an Excel spreadsheet tracking monthly expenses with formulas for totals\"</li> <li>\"Turn this meeting notes document into a PowerPoint presentation\"</li> <li>\"Generate a PDF report summarizing this data\"</li> <li>\"Build a financial model in Excel with scenario analysis\"</li> </ul> </li> </ul> </li> <li>You can upload your own files and have Claude update them. </li> <li>Security - If you're installing a custom Skill from an external source, review its contents before use to understand what it does.</li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#creating-your-own-custom-skills","title":"Creating your own custom skills","text":"<ul> <li>Create custom skills through a conversation with Claude</li> <li>Start a new chat and say, \"I want to create a skill that does X\"</li> <li>Claude will interview you about workflow, what it should do, what makes good output, and when you use the skill</li> <li>Upload any reference materials, guides, work examples, brand assets to help Claude understand</li> <li>Claude will create a skill for your to download as a ZIP file</li> <li>Upload it to your settings --&gt; capabilities</li> <li>The custom skill will automatically be invoked when needed </li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#connectors","title":"Connectors","text":"<ul> <li>Give Claude access to tools and information you use every day</li> <li>Enabling connectors allows Claude to search files, analyze documents and data, create and update content, and execute tasks from within your conversation</li> <li>MCP (model context protocol) powers connectors. MCP allows Claude to connect to many different applications from a single consistent interface. </li> <li>Two types of connectors - web and desktop. Web gives access to internet and cloud services, while desktop allows access to local files, browser control, and applications. </li> <li>Find connectors on claude.ai/directory. Add connectors by clicking on the <code>Search and Tools --&gt; connectors</code> in the chat window</li> <li>Security and access - you can toggle off/on certain permissions within each connector to give or revoke access as needed. </li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#enterprise-search","title":"Enterprise Search","text":"<ul> <li>used for finding information and data across your organization</li> <li>\"ask your ORGNAME\" option appears in the sidebar</li> <li>ask Claude about what happened recently, policies and procedures, research, analysis and work projects</li> <li>Claude searches sharepoint, email, docs, and slack to craft a response and cites its sources</li> <li>Admins must set up enterprise search, (enabled default, but it must be configured) then users authenticate with their accounts</li> <li>Users can only access what they already have permission for in their personal accounts, connected data stays private, it isn't stored or indexed by Claude</li> </ul>"},{"location":"0-inbox/AI%20notes/Claude%20Intro/#research-mode","title":"Research mode","text":"<ul> <li>Claude conducts multiple searches from different angles to give an in-depth, systematic answer to your question --&gt; agentic searching as an assistant instead of simple search</li> <li>Research takes Claude 4 to 45 minutes to complete. It uses extended thinking to plan and break tasks into manageable chunks.</li> <li>Claude provides citations to make fact checking easier. </li> <li>Research pulls information from multiple sources, compares different perspectives, and synthesizes findings in a way that enables you to act on them. </li> <li>Use research for comparative analysis and comprehensive reports that use multiple sources (web, enterprise workspaces), and quick response speed isn't necessary.  Claude can pull from your integrated tools (email, documents, slack) </li> <li>Extended thinking is best for mathematical problems, code debugging, or logical analysis instead of just information gathering. </li> <li>plan --&gt; search multiple sources --&gt; synthesize --&gt; cite</li> <li>Enable research by clicking the <code>research</code> button on the chat interface and web search must be enabled in the tools settings for research to work</li> <li>Ask Claude to help you craft a good research prompt.  Your prompt should contain your goals, the structure/sections you want and any requirements/constraints you may have. </li> <li> <pre><code> Tell me about the EV market,\" try \"Analyze the electric vehicle battery market\u2014identify key players, technology trends, and supply chain challenges that might affect investment decisions.\"\n</code></pre> </li> </ul> <pre><code> \"Compare venue options for a team offsite including: location and accessibility, meeting space and amenities, catering options, and pricing considerations.\"\n</code></pre> <p>Links: Anthropic academy</p> <p>Claude code in action</p> <p>&lt;% tp.date.now(\"YYYY-MM-DD-HHmm\") %&gt;</p>"},{"location":"1-projects/linux-backups/Create%20CloneZilla%20bootable%20USB/","title":"Create CloneZilla bootable USB","text":"<p>[[Linux]] [[backups]]</p> <p>To back up Ubuntu with Clonezilla, first create a bootable Clonezilla USB drive. Boot your computer from the USB, then select the \"device-image\" option and choose your local device (the external hard drive) as the destination. Next, select the source disk containing your system and start the backup process. </p> <ol> <li>Download Clonezilla ISO from the website.</li> <li>Insert your USB drive</li> <li>Identify your USB device name <ol> <li>Use the <code>lsblk</code> or <code>sudo fdisk -l</code> commands to identify your device name (e.g. <code>/dev/sdX*</code>). Selection the wrong device can lead to data loss! </li> </ol> </li> </ol> <pre><code>lsblk\n</code></pre> <ul> <li><code>lsblk</code>: List block devices information         <code>bash          $ sudo fdisk -l | grep -i dev</code><ul> <li><code>fdisk</code> = create and manipulate partition tables. Block devices can be divided one or more logical disks called partitions.   <code>-l</code> = list the partition tables for a device     </li> </ul> </li> <li>Unmount the USB drive    <pre><code>df -h\nsudo umount /dev/SDX*\n</code></pre></li> <li><code>df</code> = displays the amount of space on the file system</li> <li><code>umount</code> = detach a file system     </li> <li>Write the ISO to the USB drive using dd. Once completed you can boot from the USB.     <pre><code>sudo dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/dev/sdc1 status=progress bs=4M &amp;&amp; sync\n</code></pre><ul> <li>dd - convert and copy a file</li> <li>if = $INPUT_PATH: specify the input file (your ISO)</li> <li>of = $OUTPUT_PATH: specify the output file (your USB device)</li> <li>bs=4M: sets the block size to 4 MB for faster write</li> <li>&amp;&amp; sync: writes cached data to the disk before exit</li> </ul> </li> <li>Verify the ISO burned correctly by comparing the hashes of the file on disk and the USB:    <pre><code>sha256sum clonezilla-live-3.3.0-33-amd64.iso\nsudo head -c $(stat -c %s clonezilla-live-3.3.0-33-amd64.iso) /dev/sdc1 | sha256sum\n</code></pre><ul> <li><code>head</code>: outputs the first part of files.</li> <li><code>\"-c\" &lt;size&gt;</code>: This option tells <code>head</code> to output only the first <code>&lt;size&gt;</code> bytes of the input. The <code>&lt;size&gt;</code> placeholder is replaced by the exact size (in bytes) of the original ISO file. This is the crucial part that ensures only the relevant data (the image itself, not any potential extra data or padding on the USB drive) is read for the checksum calculation.</li> <li><code>/dev/sdX</code>: This is the device path for your USB drive .</li> <li><code>sha256sum</code>: Calculates and outputs the SHA256 cryptographic hash (checksum) of the data it receives through standard input   </li> </ul> </li> <li>If the checksums match, then eject the USB    <pre><code>sudo eject /dev/sdX*\n</code></pre></li> <li>Boot your computer from the USB, then select the \"device-image\" option and choose your local device (the external hard drive) as the destination. Next, select the source disk containing your system and start the backup process. </li> </ul> <pre><code>df -h\n 1949  pwd`\n 1950  dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/media/linux-joe/78E0-F73B/ status=PROGESS\n 1951  man dd\n 1952  dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/media/linux-joe/78E0-F73B/ status=progress\n 1953  dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/media/linux-joe/78E0-F73B status=progress\n 1954  ls /media/linux-joe/\n 1955  ls /media/linux-joe/78E0-F73B/\n 1956  df -h\n 1957  lsblk\n 1958  fdisk -l\n 1959  sudo fdisk -l\n 1960  dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/dev/sda1 status=progress\n 1961  sudo dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/dev/sda1 status=progress\n 1962  df -h\n 1963  ls /dev/sda1\n 1964  man dd\n 1965  mount\n 1966  sudo umount /dev/sda1\n 1967  ls -lhart clonezilla-live-3.3.0-33-amd64.iso \n 1968  ls -lhart /media/linux-joe/3.3.0-33-amd64/\n 1969  lsblk\n 1970  sudo umount /dev/sda1\n 1971  lsof +f -- /dev/sda1\n 1972  kill -9 118270\n 1973  lsof +f -- /def/sda/1\n 1974  lsof +f -- /def/sda1\n 1975  lsof +f -- /dev/sda1\n 1976  sudo umount /dev/sda1\n 1977  lsblk\n 1978  df -h\n 1979  sudo dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/dev/sda1 status=progress\n 1980  sudo dd if=/home/linux-joe/Downloads/clonezilla-live-3.3.0-33-amd64.iso of=/dev/sda1 status=progress bs=4M &amp;&amp; sync\n 1981  ls \n 1982  df -h\n 1983  lsblk\n 1984  sudo umount /dev/sda1\n 1985  lsblk\n 1986  stat -c %s clonezilla-live-3.3.0-33-amd64.iso \n 1987  stat -c %s /dev/sda1\n 1988  stat -c-n %s /dev/sda1\n 1989  stat -c -n %s /dev/sda1\n 1990  cmp -n $(stat -c %s clonezilla-live-3.3.0-33-amd64.iso) clonezilla-live-3.3.0-33-amd64.iso /dev/sda1\n 1991  sudo cmp -n $(stat -c %s clonezilla-live-3.3.0-33-amd64.iso) clonezilla-live-3.3.0-33-amd64.iso /dev/sda1\n 1992  sha256sum clonezilla-live-3.3.0-33-amd64.iso \n 1993  sudo head -c $(stat -c %s clonezilla-live-3.3.0-33-amd64.iso) /dev/sda1 | sha256sum \n 1994  sudo head -c $(stat -c %s clonezilla-live-3.3.0-33-amd64.iso) /dev/sda1\n 1995  man stat\n 1996  stat clonezilla-live-3.3.0-33-amd64.iso \n 1997  man stat\n 1998  df -h\n</code></pre>"},{"location":"1-projects/linux-backups/What%20files%20to%20include%20in%20a%20Linux%20backup%3F/","title":"What files to include in a Linux backup?","text":"<p>[[Linux]] [[backups]]</p> <p>When backing up a Linux system to a NAS, the most critical files to include are user data, system configurations, and locally installed software. A common strategy is to back up essential directories while excluding temporary or non-essential system files to save space and time. [1, 2, 3, 4, 5] Essential Directories to Include </p> <p>\u2022 /home/: This is where all user data, personal documents, downloads, and user-specific configurations are stored. Backing this up is critical.  \u2022 /etc/: Contains all system-wide configuration files, network settings, and service configurations. This is crucial for restoring your system setup.  \u2022 /var/: Contains variable data, including logs, databases, mail, and application data. Key subdirectories to consider are: </p> <pre><code>\u2022 : For local mail storage. \n\u2022 : If you host web content here. \n\u2022 : This directory often contains automatically generated package lists and other useful recovery data. \n\u2022 : Contains variable state data for applications; review subdirectories to decide what is necessary.\n</code></pre> <p>\u2022 /root/: The home directory for the root user, which may contain important administrative scripts and configurations.  \u2022 /usr/local/: Used for storing locally compiled or hand-installed software and scripts not managed by the system's package manager.  \u2022 /opt/ and /srv/: These directories may contain optional application data or service-specific files. Back up these if you have stored anything here. [1, 2, 6, 7, 8]  </p> <p>Directories to Exclude [2] Generally, you should exclude directories that contain temporary data, caches, or easily reinstallable system binaries, as including them can make backups unnecessarily large and slow. </p> <p>\u2022 /dev/: Contains device files, which are dynamically created and not needed in a backup.  \u2022 /proc/ and /sys/: Virtual filesystems that contain real-time kernel and process information, not persistent data.  \u2022 /tmp/ and /var/tmp/: Temporary scratch spaces that are cleared periodically.  \u2022 /run/ and /var/run/: Contains data relevant only to the current running system.  \u2022 /mnt/ and /media/: These are typically mount points for other drives or network shares. Backing up their contents (which are usually backed up from their original source) can lead to redundant backups or infinite loops.  \u2022 /boot/: While important, the configuration files within  and a list of installed packages are often sufficient for recovery. The kernel and initramfs images themselves can usually be reinstalled via a package manager. [1, 14, 15, 16, 17]  </p> <p>Backup Tools and Strategy  You can use various tools to perform these backups to a NAS, often mounting the NAS share using protocols like SMB or NFS. </p> <p>\u2022 Rsync: A powerful command-line tool for efficient incremental backups that only copies changed files.  \u2022 GUI Tools: Options like Back In Time or Deja Dup offer a user-friendly interface for setting up include/exclude rules.  \u2022 Best Practice: Employ the 3-2-1 backup rule: keep three copies of data, on two different types of media, with one copy stored off-site. Your NAS can serve as one of the local media types. [9, 14, 19, 20, 21]  </p> <p>AI responses may include mistakes.</p> <p>[1]\u00a0https://unix.stackexchange.com/questions/1067/what-directories-do-i-need-to-back-up [2]\u00a0https://www.zmanda.com/blog/how-to-backup-linux-directories-and-files/ [3]\u00a0https://www.zmanda.com/blog/linux-backup-a-guide-for-system-administrators/ [4]\u00a0https://www.acronis.com/en/solutions/backup/physical/linux/ [5]\u00a0https://www.itjones.com/blogs/2018/12/best-nas-network-attached-storage [6]\u00a0https://tanav2202.medium.com/linux-filesystem-and-user-permissions-62072db8df89 [7]\u00a0https://tonylixu.medium.com/devops-in-linux-var-6a22c8df65e8 [8]\u00a0https://medium.com/@ridwaneelfilali/linux-directory-hierarchy-2e7c6da8a151 [9]\u00a0https://www.youtube.com/watch?v=lR9OvVb5RvI [10]\u00a0https://www.linode.com/docs/guides/backing-up-your-data/ [11]\u00a0https://www.interserver.net/tips/kb/shared-hosting-and-backups/ [12]\u00a0https://www.truenas.com/docs/scale/scaleuireference/dataprotection/truecloudbackuptasksscreen/ [13]\u00a0https://blog.devops.dev/restic-backups-protect-your-server-before-disaster-strikes-9f76eb16a711 [14]\u00a0https://askubuntu.com/questions/222326/which-folders-to-include-in-backup [15]\u00a0https://forums.opensuse.org/t/use-smb-cifs-nas-for-backing-up-linux/143390 [16]\u00a0https://www.zmanda.com/blog/how-to-backup-linux-directories-and-files/ [17]\u00a0https://betterstack.com/community/questions/what-directories-on-linux-should-be-in-a-server-backup/ [18]\u00a0https://www.rubrik.com/insights/nas-backup-guide-for-organizations-a-comprehensive-resource [19]\u00a0https://kb.synology.com/DSM/tutorial/How_to_back_up_Linux_computer_to_Synology_NAS [20]\u00a0https://www.commvault.com/explore/nas-backup [21]\u00a0https://www.tecmint.com/linux-system-backup-tools/</p>"},{"location":"2-areas/Homelab/Homelab%20with%20kubernetes%2C%20MetalLB%20and%20BGP/","title":"Homelab with kubernetes, MetalLB and BGP","text":"<p>[[Kubernetes]] [[Metal LB]]</p> <p>https://www.growse.com/2019/04/13/at-home-with-kubernetes-metallb-and-bgp.html</p> <p>Kubernetes has a general design model of separating out capabilities from implementations - a Service will simply ask for a load balanced IP address from K8s, and it\u2019s then up to the K8s API to select an available implementation and call that to actually provide and configure that. If you\u2019re running K8s on AWS, Amazon will provide an implementation that manages ELB resources in response to the demand by the cluster. If you spin up K8s on bare metal (e.g. via <code>kubeadm</code>), then you don\u2019t get any implementations by default. However, MetalLB is an open source implementation of a k8s load-balancer, so getting set up is a simple case of \u201cdownload and install\u201d (I used helm for this).</p> <p>MetalLB presents a couple of different ways of solving the load balancer problem. The easiest approach to get set up with is \u201cLevel 2 mode\u201d, where MetalLB configures each node to announce load-balanced IPs via ARP. So a Service that configures a load-balanced IP of <code>192.168.2.100</code> will cause each of the nodes that are running pods that map to that service to send ARP announcements for <code>192.168.2.100</code> on their MAC address. Local clients (on the same subnet) who want to route traffic to <code>192.168.2.100</code> simply make an ARP request, get the first response and send packets to that ethernet address. If the node goes down, the ARP announcement is withdrawn and other clients should remove that from their cache.</p> <p>MetalLB provides an alternative, which is to use BGP to announce IPs. BGP is the common protocol widely used for routers to announce available routes to other routers, and thus coordinate available routes for IP traffic. A typical BGP announcement is (simplistically) of the form \u201cHi, Subnet <code>x</code> can be routed via IP <code>y</code> with a metric of <code>z</code>\u201d, so in the context of load-balancing, MetaLB just announces that a subnet of a single IPv4 address (a <code>/32</code>) is to be routed via the node IPv4 address that is hosting the correct pod.</p>"},{"location":"2-areas/Obsidian/Obsidian%20Shortcuts/","title":"Obsidian Shortcuts","text":"<p>ctrl + y  = open the quick explorer, 0 enter enter to be in the inbox ctrl + m = move your note to somewhere ctrl + n = new note (then move to inbox)</p> <p>ctrl + tab = go forward a new tab ctrl + shift + tab = go back a new tab</p> <p>Vim shortcuts: :3 = jump to line 3 3Y  = yank 3 lines inclusive 12dd = delete 12 lines inclusive</p> <p>[[Obsidian]]</p>"},{"location":"2-areas/Obsidian/Obsidian%20plugins/","title":"Obsidian plugins","text":"<ol> <li>Custom attachment location: </li> <li>saves attachments in an assets folder.</li> <li>Automatically moves them when you reorganize.</li> <li> <p>https://github.com/mnaoumov/obsidian-custom-attachment-location</p> </li> <li> <p>Quick switcher Quick switcher is a core plugin that lets you search and open notes using only your keyboard.</p> </li> </ol> <p>There are several ways to open Quick Switcher, when it's enabled:</p> <pre><code>Press Ctrl+O (or Cmd+O on macOS) to open the Quick switcher;\nIn the ribbon, click Open Quick switcher ( lucide-file-search.svg &gt; icon );\nOn mobile, when you're not editing a note, tap the plus icon at bottom center of the app.\n</code></pre> <ol> <li>Templater</li> <li>Allows the creation of templates for your obsidian notes</li> <li>You can set up shortcuts (Like ctrl+I) to paste in a template</li> <li>set up defaults, so creating a new note in a folder gets a default template</li> </ol> <p>[[Obsidian]]</p>"},{"location":"2-areas/PARA%20Notes/PARA%20Notes/","title":"PARA Notes","text":"<p>Projects -  Short term efforts; a collection of tasks and notes -  Collect information about a task in a folder - short-term efforts (in your work or personal life) that you take on with a certain goal in mind. For example:     - Complete webpage design     - Buy a new computer     - Write research report     - Renovate the bathroom     - Finish Spanish language course     - Set up new living room furniture Areas -  Responsible for over a longer period of time - think house maintenance -  Ongoing engagement or responsibility - You have areas of responsibility \u2013 important parts of your work and life that require ongoing attention. These might include:     - Work responsibilities such as Marketing, Human Resources, Product Management, Research and Development, Direct Reports, or Engineering     - Personal responsibilities such as Health, Finances, Kids, Writing, Car, or Home Resources - topics or interests which you expect to be useful in the future - Reference notes, things you'll come back to later - Go, Linux, vacation trips - Then you have resources on a range of topics you\u2019re interested in and learning about, such as:     - Graphic design     - Personal productivity     - Organic gardening     - Coffee     - Modern architecture     - Web design     - Japanese language     - French literature     - Notetaking     - Breathwork     - Habit formation     - Photography     - Marketing assets Archive - Inactive items which are not useful currently or in the near future, but you don't want to throw away - Navigate the vault through search, so it can be included or excluded - Finally, you have archives, which include anything from the previous three categories that is no longer active, but you might want to save for future reference:     - Projects you\u2019ve completed or put on hold     - Areas that are no longer active or relevant     - Resources that you\u2019re no longer interested in</p> <p>Links: https://fortelabs.com/blog/para/</p> <p>[[PARA Notes]]</p> <p></p> <p></p> <p>![[The PARA Method_ The Simple System for Organizing Your Digital Life in Seconds.pdf]]</p>"},{"location":"3-resources/docker_commands/","title":"commands","text":""},{"location":"3-resources/docker_commands/#ansible","title":"Ansible","text":""},{"location":"3-resources/docker_commands/#go-to-ansible-docker-file","title":"go to ansible docker file","text":"<p>cd /home/linux-joe/git/dockerfiles/ansible</p>"},{"location":"3-resources/docker_commands/#build-the-container","title":"Build the container","text":"<p>sudo docker build . -t py_ansible</p>"},{"location":"3-resources/docker_commands/#start-ansible-container","title":"Start ansible container","text":"<p>sudo docker compose run ansible</p>"},{"location":"3-resources/docker_commands/#in-the-container","title":"in the container","text":"<p>ansible-playbook -i nuc_inventory playbooks/hello_world.yml ansible-playbook -i nuc_inventory playbooks/</p>"},{"location":"3-resources/docker_commands/#sudoers-and-using-become","title":"Sudoers and using become","text":"<p>ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/shutdown.yml ansible-playbook -i nuc_inventory --ask-vault-pass --extra-vars '@passwd.yml' playbooks/nuc_config_playbook.yml ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/nuc_config_playbook.yml ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/k8s_prereqs_playbook.yml ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/k8s_prereqs_playbook.yml --tags \"waka\" ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' playbooks/k8s_resources.yml -vv</p>"},{"location":"3-resources/docker_commands/#siteyml-and-limit-to-2-nodes","title":"site.yml and limit to 2 nodes","text":"<p>ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' site.yml -l nuc1,nuc4  -vv ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' site.yml -l nuc1,nuc4  -vv --check ansible-playbook -i nuc_inventory --extra-vars '@passwd.yml' site.yml -vv</p>"},{"location":"3-resources/docker_commands/#kubectl-commands","title":"Kubectl commands","text":"<p><code>kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code> INGRESS_EXTERNAL_IP=<code>kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code></p> <ul> <li>metal lb tutorial: https://www.adaltas.com/en/2022/09/08/kubernetes-metallb-nginx/</li> </ul> <p>kubectl -n metallb-system describe controller-7499d4584d-28ccw kubectl -n metallb-system get secret metallb-webhook-cert -ojsonpath='{.data.ca.crt}' kubectl -n metallb-system get secret metallb-webhook-cert -ojsonpath='{.data.ca.crt}' | base64 -d kubectl -n metallb-system get secret metallb-webhook-cert -ojsonpath='{.data.ca.crt}' | base64 -d &gt; caBundle.pem kubectl -n metallb-system get secret webhook-server-cert -ojsonpath='{.data.ca.crt}' kubectl -n metallb-system get secret webhook-server-cert -ojsonpath='{.data.ca.crt}' | base64 -d kubectl -n metallb-system get secret webhook-server-cert -ojsonpath='{.data.ca.crt}' | base64 -d &gt; caBundle.pem kubectl create deploy nginx --image nginx kubectl expose deploy nginx --port 80 --type LoadBalancer kubectl delete deployment nginx kubectl delete pod -n kube-flannel -l app=flannel kubectl delete pod -n metallb-system kubectl delete pod -n metallb-system -l app=metallb kubectl delete pod -n metallb-system -l metallb kubectl delete svc nginx kubectl describe crd ipaddresspools.metallb.io kubectl describe nodes kubectl describe nodes nuc1 kubectl describe pod test-pod-nginx kubectl describe pods kubectl describe pods --all-namespaces kubectl describe pods -n default kubectl describe pods -n default nginx-7854ff8877-j292l kubectl describe pods -n default nginx-7854ff8877-pfvhq kubectl describe pods -n kube-flannel kubectl describe pods -n metallb-system kubectl describe service nginx kubectl describe svc kubectl describe svc kubernetes-dashboard kubectl describe svc metallb kubectl describe svc metallb-system kubectl describe svc metallb-webhook-service kubectl describe svc metallb-webhook-service -n metallb-system kubectl edit configmap kubectl get all kubectl get all --all-namespaces kubectl get crd kubectl get endpoints -n metallb-system kubectl get endpoints -n metallb-system~ kubectl get events --sort-by=.metadata.creationTimestamp kubectl get logs -n kube-flannel kubectl get logs -n kube-flannel kube-flannel-ds-8755f kubectl get namespace kubectl get nodes kubectl get nodes --help kubectl get nodes --show-label kubectl get nodes --show-labels kubectl get nodes -o wide kubectl get pod --all-namespaces kubectl get pods kubectl get pods --all-namespaces kubectl get pods -n default kubectl get pods -n metallb-system kubectl get secrets kubectl get secrets -n metallb-system kubectl get service -n metallb-system kubectl get service -n metallb-system  metallb-webhook-service kubectl get service -n metallb-system  webhook-service kubectl get svc kubectl get svc --help kubectl get svc nginx kubectl get svc nginx -o kubectl get svc nginx -o json kubectl get svc nginx -o jsonpath kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip} kubectl get svc nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' kubectl get svc nginx -o jsonpath='{.status}' kubectl get validatingwebhookconfiguration metallb-webhook-configuration -ojsonpath='{.webhooks[0].clientConfig.caBundle}' kubectl get validatingwebhookconfiguration metallb-webhook-configuration -ojsonpath='{.webhooks[0].clientConfig.caBundle}' kubectl get validatingwebhookconfiguration metallb-webhook-configuration -ojsonpath='{.webhooks[0].clientConfig.caBundle}' | base64 -d kubectl list svc kubectl log nginx-7854ff8877-j292l kubectl logs -n kube-flannel kube-flannel-ds-8755f kubectl logs nginx-7854ff8877-j292l kubectl logs test-pod-nginx kubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8080:443 kubectl proxy kubectl run -i --tty --rm test-pod-nginx --image=nginx --restart=Never --namespace default</p>"},{"location":"3-resources/docker_commands/#expose-a-running-service-with-the-load-balancer","title":"expose a running service with the load balancer","text":"<p>k -n kubernetes-dashboard expose service kubernetes-dashboard-web --type=LoadBalancer --name=kubernetes-dashboard-svc</p> <p>enable kube dashboard with https so token works: (edit the the kong proxy service to have a nodeport and go directly to i, circumvent the loadbalancer ip)</p> <p>https://github.com/kubernetes/dashboard/issues/9066#issuecomment-2254511968 dashboard IP with kong modified: https://192.168.1.47:32260/ dashboard via svc external ip: 192.168.1.241:8000</p>"},{"location":"3-resources/docker_commands/#start-here","title":"START HERE","text":"<p>Why the difference?  Kong is circumventing tls, but how, it is able to go directly to web? Need to understand traffic flow into k8s pods.</p>"},{"location":"3-resources/docker_commands/#helm","title":"helm","text":"<p>100  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/   102  /usr/local/bin/helm list --output=yaml   103  /usr/local/bin/helm list --output=yaml --filter kube-dashboard   104  /usr/local/bin/helm list --output=yaml --filter kubernetes-dashboard   105  helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard   106  helm delete kubernetes-dashboard --namespace kubernetes-dashboard</p>"},{"location":"3-resources/docker_commands/#ansible-vault","title":"Ansible vault","text":"<p>To create a new encrypted data file, run the following command:</p> <p><code>ansible-vault create foo.yml</code></p> <p>To edit an encrypted file in place, use the ansible-vault edit command. This command will decrypt the file to a temporary file and allow you to edit the file, saving it back when done and removing the temporary file:</p> <p><code>ansible-vault edit foo.yml</code></p>"},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/","title":"To expose the Kubernetes Dashboard using MetalLB, you need to follow a few steps. Here's a guide on how to set it up","text":""},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/#step-1-install-metallb-if-not-already-installed","title":"Step 1: Install MetalLB (if not already installed)","text":"<p>First, ensure that MetalLB is installed and running on your Kubernetes cluster.</p> <ol> <li>Apply the MetalLB manifest:</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.3/manifests/metallb.yaml\n</code></pre> <ol> <li>Create a ConfigMap for MetalLB to use the address pool <code>my-test-app</code> that you created:</li> </ol> <p>Here's an example <code>ConfigMap</code> for MetalLB. Replace the IP range with the appropriate range you set for the <code>my-test-app</code> pool:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: metallb-config\n  namespace: metallb-system\ndata:\n  config: |\n    address-pools:\n    - name: my-test-app\n      protocol: layer2\n      addresses:\n      - 192.168.1.240-192.168.1.250  # Use the range from your address pool\n</code></pre> <p>Apply this config with:</p> <pre><code>kubectl apply -f metallb-config.yaml\n</code></pre>"},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/#step-2-deploy-the-kubernetes-dashboard","title":"Step 2: Deploy the Kubernetes Dashboard","text":"<p>If you haven't already deployed the Kubernetes Dashboard, you can deploy it using the following command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n</code></pre>"},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/#step-3-create-a-service-to-expose-the-dashboard","title":"Step 3: Create a Service to Expose the Dashboard","text":"<p>Next, you need to expose the Kubernetes Dashboard through a LoadBalancer service that MetalLB can assign an IP to.</p> <p>Create a <code>Service</code> definition for the Kubernetes Dashboard as follows:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  selector:\n    k8s-app: kubernetes-dashboard\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9090\n  type: LoadBalancer\n</code></pre> <p>Apply the service:</p> <pre><code>kubectl apply -f dashboard-service.yaml\n</code></pre>"},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/#step-4-verify-the-loadbalancer-ip","title":"Step 4: Verify the LoadBalancer IP","text":"<p>MetalLB will assign an IP from the <code>my-test-app</code> address pool to the <code>kubernetes-dashboard</code> service.</p> <p>You can check the assigned IP with:</p> <pre><code>kubectl get svc -n kubernetes-dashboard\n</code></pre> <p>You should see the <code>EXTERNAL-IP</code> populated with an IP address from the <code>my-test-app</code> pool.</p>"},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/#step-5-access-the-dashboard","title":"Step 5: Access the Dashboard","text":"<p>Once the service is exposed and the IP address is assigned, you can access the Kubernetes Dashboard by visiting the assigned IP address in your browser.</p> <p>If you use port 80 in the service, just navigate to:</p> <pre><code>http://&lt;assigned-ip&gt;\n</code></pre>"},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/#step-6-set-up-the-access-token","title":"Step 6: Set Up the Access Token","text":"<p>To log in to the Kubernetes Dashboard, you'll need to create a ServiceAccount and ClusterRoleBinding for the necessary permissions:</p> <pre><code>kubectl create serviceaccount dashboard-sa -n kubernetes-dashboard\nkubectl create clusterrolebinding dashboard-sa-binding \\\n  --clusterrole=cluster-admin \\\n  --serviceaccount=kubernetes-dashboard:dashboard-sa\n</code></pre> <p>Get the token for the <code>dashboard-sa</code> ServiceAccount:</p> <pre><code>kubectl -n kubernetes-dashboard create token dashboard-sa\n</code></pre> <p>This token can be used to log in to the Kubernetes Dashboard.</p>"},{"location":"3-resources/kube/Install_k8s_dash_board_metallb/#summary","title":"Summary","text":"<ol> <li>Install and configure MetalLB.</li> <li>Deploy the Kubernetes Dashboard.</li> <li>Create a LoadBalancer service to expose the dashboard using the <code>my-test-app</code> address pool.</li> <li>Retrieve the assigned IP address.</li> <li>Log in to the dashboard with the generated access token.</li> </ol> <p>Let me know if you need further clarification!</p>"},{"location":"3-resources/kube/K8s%20CNI%20and%20Ingress%20Controller%20Comparison/","title":"K8s CNI and Ingress Controller Comparison","text":"<p>[[Kubernetes]] [[Ingress controller]] [[Metal LB]]</p> <p>Can you give me a comparision of k8s CNI flannel, calico, cilium and include ingress controller functionality</p> <p>Choosing a CNI depends on whether you value</p> <p>simplicity, traditional enterprise security, or modern eBPF-powered observability.\u00a0</p> <p>Comparison Overview</p> Feature Flannel Calico Cilium Primary Goal Simplest L3 networking Performance &amp; policy Security &amp; observability Technology VXLAN, UDP, Host-GW BGP, IPIP, VXLAN, eBPF eBPF Security None (Basic connectivity) L3/L4 Network Policies L3\u2013L7 Policies + Identity Performance High (Low overhead) High (Direct routing) High (Bypasses iptables) Complexity Very Low Moderate High <p>Ingress Controller Functionality</p> <ul> <li>Flannel: Does not include an ingress controller. You must deploy a separate solution like NGINX Ingress Controller or HAProxy.</li> <li>Calico: Offers a Calico Ingress Gateway (part of their enterprise/cloud features) that leverages Envoy. It provides fine-grained RBAC and integrated security policies for incoming traffic.</li> <li>Cilium: Features a built-in, fully-conformant Cilium Ingress Controller powered by eBPF and Envoy. It supports shared or dedicated load balancers and integrates directly with Cilium's L7 security policies and Hubble observability.\u00a0</li> </ul> <p>Summary Recommendation</p> <ul> <li>Flannel: Best for small, simple clusters where security policies aren't required.</li> <li>Calico: Best for large-scale, multi-OS environments needing rock-solid L3/L4 security.</li> <li>Cilium: Best for modern, high-performance clusters requiring deep visibility (L7) and a unified networking/ingress stack.\u00a0</li> </ul> <p>These technical guides compare Flannel, Calico, and Cilium Kubernetes networking solutions, detailing their features, security, performance, complexity, and ingress controller capabilities:</p> <p></p> <p>Calico vs. Cilium: 9 Key Differences and How to Choose - Tigera</p> <p>Calico and Cilium: Ideal Use Cases. Calico and Cilium are both suited for environments needing scalable and secure networking for ...</p> <p></p> <p>Tigera</p> <p></p> <p></p> <p>Navigating the Ingress-nginx Archival: Why Now Is the Time to Move ...</p> <p>Nov 13, 2025 \u2014 Option 1 - Quickest: Moving to Cilium Ingress. Cilium Ingress is a drop-in ingress controller that supports standard Kubernetes In...</p> <p></p> <p>Isovalent</p> <p></p> <p></p> <p>Calico vs. Cilium: Which Kubernetes CNI Is Right for You?</p> <p>Both Calico and Cilium are excellent choices, but they shine in different contexts. Calico is stable, flexible, and great for orga...</p> <p></p> <p>zesty.co</p> <p></p> <p></p> <p>Cilium vs Calico vs Flannel - Civo.com</p> <p>Jul 17, 2023 \u2014 Performance characteristics and scalability. ... Calico's direct IP-to-IP routing provides high performance and low latency, makin...</p> <p></p> <p>Civo</p> <p></p> <p></p> <p>Comparing Kubernetes CNI Providers: Flannel, Calico, Canal ...</p> <p>Sep 24, 2021 \u2014 Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsula...</p> <p></p> <p>SUSE</p> <p></p> <p>Stop Using the Wrong CNI in 2026: Flannel vs Calico vs Cilium</p> <p>Aug 3, 2025 \u2014 If you just need connectivity in a small cluster: Stick with Flannel. If you want something secure and production\u2011like, without go...</p> <p></p> <p>DevOps.dev</p> <p></p> <p>Would you like a more detailed breakdown of how to configure Cilium's ingress for your specific cluster setup?</p> <ul> <li> <p></p> <p>Stop Using the Wrong CNI in 2026: Flannel vs Calico vs Cilium</p> <p>Aug 3, 2025 \u2014 If you just need connectivity in a small cluster: Stick with Flannel. If you want something secure and production\u2011like, without go...</p> <p></p> <p>DevOps.dev</p> <p></p> </li> <li> <p></p> <p>Cilium vs Calico vs Flannel - Civo.com</p> <p>Jul 17, 2023 \u2014 Performance characteristics and scalability. ... Calico's direct IP-to-IP routing provides high performance and low latency, makin...</p> <p></p> <p>Civo</p> <p></p> </li> <li> <p></p> <p>The Ultimate Guide To Using Calico, Flannel, Weave and Cilium</p> <p>Jun 7, 2021 \u2014 Selecting the Right Solution For Your Project. Depending on your needs, selecting the right CNI plugin to use in your cluster coul...</p> <p></p> <p>Platform9</p> <p></p> </li> </ul> <p>Show all</p> <p>Keep this information and add metal LB to the list</p> <p>Adding</p> <p>MetalLB to the comparison shifts the focus toward bare-metal environments where cloud-based load balancers (like AWS ELB or GCP Cloud LB) are unavailable.\u00a0</p> <p>Updated Comparison Table</p> Feature Flannel Calico Cilium MetalLB Primary Goal Simple L3 networking Secure L3/L4 networking eBPF-powered L3\u2013L7 security External IP for Bare-Metal Role CNI CNI CNI External Load Balancer Technology VXLAN, UDP BGP, IPIP, VXLAN eBPF L2 (ARP) or L3 (BGP) Security None Rich L3/L4 Network Policies L3\u2013L7 Policies + Identity Traffic announcement only Complexity Very Low Moderate High Low (L2) to Moderate (BGP) <p>Ingress Controller &amp; MetalLB Interaction</p> <p>Unlike the CNIs mentioned, MetalLB is not a CNI and does not provide an Ingress Controller on its own. Instead, it serves as the bridge that allows your Ingress Controller to be reachable from outside the cluster.\u00a0</p> <ul> <li>Relationship: In a cloud environment, creating an Ingress often triggers a cloud load balancer to get an IP. In bare-metal, MetalLB performs this role by assigning a \"floating\" IP from a pre-configured pool to your Ingress Controller's service.</li> <li>Integration with Flannel/Calico: These CNIs handle pod-to-pod traffic, while MetalLB handles the \"north-south\" entry point. You still need a separate Ingress Controller (like NGINX or Traefik).</li> <li>Integration with Cilium: Cilium has a built-in Ingress Controller. In a bare-metal setup, you can use MetalLB to provide the stable external IP for that Cilium Ingress.</li> <li>Unique Note: Calico and MetalLB both use BGP. If you use them together, it is often recommended to let Calico handle BGP and run MetalLB in Layer 2 mode to avoid protocol conflicts.\u00a0</li> </ul> <p>Summary Recommendation</p> <ul> <li>MetalLB: Essential if you are running on-premise/bare-metal and want your services to have a stable external IP (Service type: <code>LoadBalancer</code>).</li> <li>The \"Pro\" Stack: Pair Cilium (for high-performance CNI and Ingress) with MetalLB (to provide the external IP) for a modern, production-grade bare-metal cluster.\u00a0</li> </ul> <p>These articles explain the role of MetalLB as an external load balancer for bare-metal Kubernetes clusters and its integration with CNIs like Calico and Cilium.</p> <p></p> <p>Ingress vs Load Balancers (MetalLB) : r/kubernetes - Reddit</p> <p>Jun 3, 2025 \u2014 * geth2358. \u2022 8mo ago. MetalLB is the mean, Ingress is one user of this mean. MetalLB controls the input traffic at layer 2. This ...</p> <p></p> <p>Reddit</p> <p></p> <p>Choosing the Right Kubernetes CNI in 2025: Flannel, Calico ...</p> <p>Sep 14, 2025 \u2014 1. Flannel: The Safe Default. Strengths: It's simple and ubiquitous\u2014often the default in lightweight distributions like K3s, offer...</p> <p></p> <p>Medium</p> <p></p> <p>Concepts :: MetalLB, bare metal load-balancer for Kubernetes</p> <p>Concepts. MetalLB hooks into your Kubernetes cluster, and provides a network load-balancer implementation. In short, it allows you...</p> <p></p> <p>MetalLB</p> <p></p> <p></p> <p>MetalLB basic knowledge ! - MyLearnings!</p> <p>Apr 9, 2024 \u2014 Here's when and why you might use MetalLB: * Bare-metal Kubernetes Clusters: In bare-metal Kubernetes environments, there may not ...</p> <p></p> <p>Hashnode</p> <p></p> <p></p> <p>Kubernetes Ingress Support \u2014 Cilium 1.19.0 documentation</p> <p>Feb 4, 2026 \u2014 How Cilium Ingress and Gateway API differ from other Ingress controllers. One of the biggest differences between Cilium's Ingress ...</p> <p></p> <p>Cilium Docs</p> <p></p> <p></p> <p>Ingresses and Load Balancers in Kubernetes with MetalLB ...</p> <p>Sep 8, 2022 \u2014 They redistribute the traffic load of the cluster, providing an effective way to prevent node overload. They allow for a single po...</p> <p></p> <p>Adaltas</p> <p></p> <p>Would you like help with a BGP vs. Layer 2 configuration guide for MetalLB in your specific network?</p>"},{"location":"3-resources/linux/APT%20commands/","title":"APT commands","text":""},{"location":"3-resources/linux/APT%20commands/#links","title":"Links","text":"<p>1.03-installing-software</p>"},{"location":"3-resources/linux/APT%20commands/#3-key-points","title":"3 Key Points","text":""},{"location":"3-resources/linux/APT%20commands/#summary","title":"Summary","text":"<p><code>sudo apt update &amp;&amp; sudo apt upgrade</code> - download list of available packages and upgrade installed ones to the latest version</p> <p><code>apt search htop</code> - search for packages  <code>apt show htop</code> - show package information <code>sudo apt install htop</code> - installs a package</p> <p><code>sudo apt remove htop</code> - remove package, but keep config files <code>sudo apt purge htop</code> - remove package and config files</p> <p><code>apt list --installed</code> - show installed packages <code>apt list --installed | grep htop</code> - filter installed packages</p>"},{"location":"3-resources/linux/APT%20commands/#further-reading","title":"Further Reading","text":""},{"location":"3-resources/linux/APT%20commands/#tags","title":"Tags","text":"<p>2026-02-26-0649</p>"},{"location":"3-resources/linux/Linux%20Boot%20Process/","title":"Linux Boot Process","text":"<p>[[Linux]] [[systemd]]</p> <ol> <li>Power on -&gt; BIOS (basic input/output system) or UEFI (unified extensible firmware interface) is loaded from disk and executes POST (power on self test).</li> <li>BIOS detects devices connected to the system -&gt; CPU, RAM, storage</li> <li>Boot device is chosen to book the OS, hard drive, network, usb, </li> <li>BIOS starts the bootloader (GRUB), which starts the kernel</li> <li>When the kernel is ready, system switches to the user space and starts up Systemd init as the first process. Init manages the processes and services, starts other hardware, mounts the file systems, starts desktop</li> <li>Systemd starts default target units</li> <li>Systemd runs startup scripts and configures the environment. </li> <li>Users receive a login window and system is ready</li> </ol> <p>https://bytebytego.com/guides/linux-boot-process-explained/</p> <p>![[Pasted image 20260214144812.png]]</p>"},{"location":"3-resources/linux/Submitting%20your%20first%20Kernel%20Patch/","title":"Submitting your first Kernel Patch","text":"<p>[[Linux]] [[Kernel]]</p> <p>https://kernelnewbies.org/FirstKernelPatch</p> <p>This tutorial will cover how to get your first patch submitted. You have three choices for how to complete this tutorial:</p> <pre><code>Run Linux in VMPlayer from Windows.\nRun Linux natively on your own machine.\nRun Linux within VMPlayer on Linux.\n</code></pre> <p>We recommend running Linux natively. Most Linux kernel developers run Linux natively, so you may as well get used to it. :) If you want to run Linux in VMPlayer, follow these directions. Note, you will not be able to compile the Linux kernel on a Mac, because the filesystem defaults to case-insensitive.</p> <p>This tutorial assumes you are running Ubuntu or Debian. If you are running Fedora, Suse, Arch, or Gentoo, the package installation commands or package names may be slightly different. Ask for help on #kernelnewbies on irc.oftc.net if you get stuck. If (and only if) you are an applicant for Outreachy, then you should ask for help on #kernel-outreachy on irc.oftc.net.</p> <p>Additionally, we highly recommend that applicants have a stable internet connection, with no download caps. Communication over IRC can be difficult if your internet connection keeps dropping or has a big lag time, so you need a stable internet connection. Downloading the initial kernel will use over 5 GB of data, which will easily blow through a standard 3G capped plan. We recommend making sure you have cable internet, or an unlimited 3G plan. </p>"},{"location":"3-resources/linux/What%20happens%20when%20you%20press%20ctrl%5Ec/","title":"What happens when you press ctrl^c","text":"<p>[[Linux]]</p>"},{"location":"3-resources/linux/What%20happens%20when%20you%20press%20ctrl%5Ec/#what-happens-when-you-press-crtlc","title":"What happens when you press CRTL^C?","text":"<p>When you press Ctrl-C on your local machine, the kernel sends a SIGINT signal (Signal interrupt)  to the foreground process group of the terminal. The terminal driver is responsible for translating key presses into signals and passing them to the correct process group. The default action for SIGINT is to terminate the process, but the process can catch the signal and handle it in a custom way.</p> <p>When you press Ctrl-C in a terminal, the terminal emulator writes an ASCII character (^C) to the master device. The kernel translates that into sending a SIGINT signal to the foreground process group with the corresponding controlling terminal.  This is actually a default terminal setting. You can run <code>stty -a</code> and see that the default is <code>intr = ^C</code>;, meaning ^C or ETX is the \"SIGINT\" character.</p> <p><code>bash line-numbers highlight=3 linux-joe@x1:$ stty -a speed 38400 baud; rows 24; columns 80; line = 0; intr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = &lt;undef&gt;; eol2 = &lt;undef&gt;; swtch = &lt;undef&gt;; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R;...</code></p> <p>A more complex example would be how Ctrl-C works through an interactive SSH session. Interactive SSH sessions allocate a pty on the server side. The client side pty is set to raw mode, meaning that the client side kernel will not translate ETX into SIGINT. Instead, the client side kernel passes the ETX along to the slave. In this case, the ssh client process takes that ETX and passes it along to the server sshd process. If the server sshd pty is not in raw mode, then the server's kernel will translate that ETX into a SIGINT to its foreground process group. This is how Ctrl-C sends SIGINT to the process running on the server instead of killing your client side SSH and leaving you hanging.</p>"},{"location":"3-resources/linux/What%20happens%20when%20you%20press%20ctrl%5Ec/#components","title":"Components","text":""},{"location":"3-resources/linux/What%20happens%20when%20you%20press%20ctrl%5Ec/#signals","title":"Signals","text":"<p>Signals are a protocol for interrupting and closing programs from the outside. There are a few different types of signals, and they all do different things.</p> <p>Some signals, like <code>SIGKILL</code> and <code>SIGSTOP</code>, cannot be caught or ignored by the program. They will always kill or stop the program.</p> <p>Other signals can be caught by the program and handled in a custom way. For example, Ctrl+C in the terminal almost always sends SIGINT, however, a program can catch <code>SIGINT</code> and do something other than exit when it receives that signal.</p>"},{"location":"3-resources/linux/What%20happens%20when%20you%20press%20ctrl%5Ec/#terminal-this-needs-editing","title":"terminal - This needs editing","text":"<p>what is a pty? A pty is a pseudo-terminal. It is a pair of devices that provide a bidirectional communication channel. One device is the master, and the other is the slave. The master device is responsible for translating key presses into signals and passing them to the slave device. The slave device is responsible for reading key presses and writing output.</p> <p>When you run a program in a terminal, the terminal emulator creates a pty pair and runs the program with the slave device as its controlling terminal. The terminal emulator then reads key presses from the master device and writes them to the slave device. The program reads key presses from the slave device and writes output to the master device. The terminal emulator reads output from the master device and displays it on the screen.</p>"},{"location":"3-resources/linux/What%27s%20the%20point%20of%20etc%20hosts%3F/","title":"What's the point of etc hosts?","text":"<p>[[Linux]] [[dns]]</p> <p>How does /etc/hosts work?</p> <p>The concept of the /etc/hosts file is very simple, just an address and a host name:</p> <p>127.0.0.1      localhost</p> <p>for each line. That is a simple list of pairs of address-host.2</p> <p>Its primary present-day use is to bypass DNS resolution. A match found in the /etc/hosts file will be used before any DNS entry. In fact, if the name searched (like localhost) is found in the file, no DNS resolution will be performed at all.</p> <p>1 Well, the order of name resolution is actually defined in /etc/nsswitch.conf, which usually has this entry:</p> <p>hosts:          files dns</p> <p>which means \"try files (/etc/hosts); and if it fails, try DNS.\"</p> <p>But that order could be changed or expanded.</p> <p>The hosts file contains lines of text consisting of an IP address in the first text field followed by one or more host names. Each field is separated by white space \u2013 tabs are often preferred for historical reasons, but spaces are also used. Comment lines may be included; they are indicated by an octothorpe (#) in the first position of such lines. Entirely blank lines in the file are ignored. For example, a typical hosts file may contain the following:</p> <pre><code>127.0.0.1   localhost loopback\n::1         localhost localhost6 ipv6-localhost ipv6-loopback mycomputer.local\n192.168.0.8 mycomputer.lan\n10.0.0.27   mycomputer.lan\n</code></pre> <p>This example contains entries for the loopback addresses of the system and their host names, the first line is a typical default content of the hosts file. The second line has several additional (probably only valid in local systems) names. The example illustrates that an IP address may have multiple host names (localhost and loopback), and that a host name may be mapped to both IPv4 and IPv6 IP addresses, as shown on the first and second lines respectively. One name (<code>mycomputer.lan</code>) may resolve to several addresses (<code>192.168.0.8 10.0.0.27</code>). However, in that case, which one is used depends on the routes (and their priorities) set for the computer.</p>"},{"location":"3-resources/linux/contex_switching/","title":"Contex switching","text":"<p>[[Linux]]</p>"},{"location":"3-resources/linux/contex_switching/#context-switching-in-operating-systems-a-detailed-explanation","title":"Context Switching in Operating Systems: A Detailed Explanation","text":"<p>How does the kernel handle context switching efficiently.</p>"},{"location":"3-resources/linux/contex_switching/#scenario-context-switch-between-task-a-and-task-b","title":"Scenario: Context Switch Between Task A and Task B","text":"<p>I walk through a context switch in a Linux system where the kernel switches from Task A to Task B</p> <p>Focus:</p> <ul> <li>Kernal's role in managing context switches</li> <li>Isolation and security between tasks</li> </ul>"},{"location":"3-resources/linux/contex_switching/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<p>Key components:</p> <p>Scheduler: what is the scheduler in linux?: The scheduler is an operating system component that determines which tasks run on the CPU and when the tasks run. It is responsible for managing the CPU\u2019s time efficiently and fairly. The scheduler uses various algorithms to decide what is fair (i.e. which task to run next), based on factors like task priority, time slices (i.e. alloted time), and other scheduling policies.</p> <p>Process control block: The Process Control Block (PCB) is a data structure used by the operating system to manage information about a process. It contains information such as the process ID, process state (i.e. stack pointer), the next instruction to execute (i.e. program counter), CPU registers, and other details needed to manage and control the process.</p> <p>Program Counter: The program counter (PC) is a register in the CPU that stores the address of the next instruction to be executed. It is a key component in the context switch process, as it determines where the CPU should resume execution for a seamless transition. During a context switch, the Program counter is saved and restored to ensure that the task resumes execution correctly.</p> <p>Stack pointer: The stack pointer (SP) is a register in the CPU that points to the top of the stack. The stack is used to store function calls, local variables, and return addresses. During a context switch, the stack pointer is saved and restored to ensure that the task resumes execution correctly.</p>"},{"location":"3-resources/linux/contex_switching/#1-triggering-the-context-switch","title":"1. Triggering the Context Switch","text":"<ul> <li> <p>When it happens: A context switch is triggered when the operating system decides that the currently running task (Task A) has either used up its time slice, is waiting for some I/O operation, or is blocked for some other reason (e.g., waiting for a resource to become available). Switching also happen when a higher-priority task becomes ready to run or when an interrupt occurs (like a hardware interrupt from a device).</p> </li> <li> <p>The Role of the Scheduler: The kernel\u2019s scheduler decides when a context switch happens. The scheduler\u2019s role is to ensure fair distribution of CPU time among tasks and to respect their priority levels. It decides which task should run next based on factors like task priority, time slices, and other internal scheduling algorithms (e.g., round-robin, priority-based scheduling).</p> </li> </ul>"},{"location":"3-resources/linux/contex_switching/#2-saving-task-as-state","title":"2. Saving Task A\u2019s State","text":"<ul> <li> <p>Pausing Task A: The kernel halts Task A\u2019s execution when the scheduler decides to switch. The key here is to preserve the task\u2019s state so that it can continue seamlessly from where it left off.</p> </li> <li> <p>What gets saved: The kernel saves the following state of Task A:</p> </li> <li>CPU Registers: These are small, fast, temporary storage areas used by the CPU to hold values that the task is currently working with (like data, intermediate calculations, pointers to memory, etc.). They are also known as processor registers or memory registers<ul> <li>Examples of CPU registers include the program counter (PC), stack pointer (SP), and general-purpose registers like RAX, RBX, RCX, etc.</li> <li>The registers hold critical information about the task\u2019s execution state</li> <li>Saving and restoring these registers is essential for the task to resume correctly</li> </ul> </li> <li>Each task has its own set of registers, so when switching, the current set of registers needs to be saved to avoid overwriting the task's data.</li> <li>See here for more info on CPU Registers</li> <li>Program Counter (PC): The program counter stores the address of the next instruction that the CPU will execute. This allows the task to resume from the exact point it was interrupted.</li> <li>Stack Pointer (SP): The stack pointer tracks the task\u2019s current position on its stack. The stack is used to store function calls, local variables, and return addresses. This ensures that when Task A resumes, its function calls and local variables are intact.</li> <li> <p>Additional Context: In some cases, the kernel may also save the task\u2019s floating-point or vector registers if the task uses advanced CPU features like SIMD instructions (e.g., for mathematical computations).</p> </li> <li> <p>Where the state is saved: The saved context is typically stored in memory within task_struct (or thread_info) structures. These are kernel-managed data structures that represent each task. Each task has its own <code>task_struct</code>, which contains information like the saved CPU registers, process IDs, the task\u2019s scheduling details, and more.</p> </li> </ul>"},{"location":"3-resources/linux/contex_switching/#to-do-what-are-the-commands-to-see-the-task-struct","title":"TO DO: ^^ What are the commands to see the task struct?? ^^","text":""},{"location":"3-resources/linux/contex_switching/#3-choosing-task-b-to-run","title":"3. Choosing Task B to Run","text":"<ul> <li>Scheduler\u2019s Decision: After saving Task A\u2019s state, the scheduler selects the next task to run. In this case, it chooses Task B.</li> <li>The decision could be based on Task B\u2019s priority (if it\u2019s a higher-priority task) or because Task B has become ready to run (for example, after waiting for some resource).</li> <li> <p>The scheduler uses scheduling policies to decide which task gets the CPU next, balancing fairness with efficiency. Common algorithms include round-robin scheduling, priority-based scheduling, and multilevel feedback queues.</p> </li> <li> <p>Choosing the Next Task Efficiently: The scheduler maintains a list (or queue) of ready tasks. It selects the next task based on priority and other factors, minimizing wasted CPU time. The kernel uses preemptive multitasking, meaning the scheduler can interrupt and switch tasks at any time (even within the middle of a task\u2019s execution).</p> </li> </ul>"},{"location":"3-resources/linux/contex_switching/#4-restoring-task-bs-state","title":"4. Restoring Task B\u2019s State","text":"<ul> <li>Loading Task B\u2019s State: The kernel now restores Task B\u2019s state, allowing it to resume execution. This includes:</li> <li>Restoring CPU Registers: The kernel loads the saved register values from Task B\u2019s state into the CPU registers. These registers include temporary data (e.g., values for ongoing calculations or state tracking) that Task B was working with before being paused.</li> <li>Restoring Program Counter (PC): The program counter is updated to the saved address where Task B left off. This is crucial for ensuring that Task B resumes at the correct place in its execution flow, without skipping any instructions or repeating previous ones.</li> <li> <p>Restoring Stack Pointer (SP): The kernel also updates the stack pointer to where Task B\u2019s stack was when it was paused. This ensures that Task B\u2019s function calls and local variables are restored correctly, and that it continues its execution on the correct stack frame.</p> </li> <li> <p>Isolation: Each task has its own isolated memory space. The kernel ensures that one task cannot interfere with the memory or state of another task. This isolation is fundamental for security and stability. It prevents tasks from accessing each other\u2019s data, which could lead to data corruption or security vulnerabilities. In a multitasking environment, tasks should be isolated, and the kernel provides mechanisms like virtual memory and memory protection to achieve this.</p> </li> <li> <p>Switching Between Tasks Securely: To prevent malicious or poorly-behaved tasks from manipulating other tasks\u2019 data, the kernel enforces strict memory protections and process boundaries. Virtual memory allows each task to believe it has its own private memory, even though multiple tasks may be running on the same physical machine.</p> </li> </ul>"},{"location":"3-resources/linux/contex_switching/#5-context-switch-completion","title":"5. Context Switch Completion","text":"<ul> <li> <p>Task B Resumes Execution: After restoring Task B\u2019s state, the kernel gives control of the CPU to Task B. Task B begins executing from the point where it left off, as if it had been running continuously.</p> </li> <li> <p>Efficient Execution: The kernel\u2019s job is to ensure that the context switch itself is minimal in overhead. It strives to switch tasks as efficiently as possible, reducing the time spent switching tasks. Context switches need to be quick, or else the system would waste too much time switching between tasks instead of actually running them.</p> </li> </ul>"},{"location":"3-resources/linux/contex_switching/#6-task-b-running-and-returning-to-task-a","title":"6. Task B Running and Returning to Task A","text":"<ul> <li>Task B Runs: Task B continues to run until one of the following happens:</li> <li>It completes its execution.</li> <li>It gets blocked (waiting for I/O or some other event).</li> <li> <p>It consumes its time slice, and the scheduler decides to switch to another task (possibly back to Task A).</p> </li> <li> <p>Returning to Task A: If Task A needs to be resumed, the kernel repeats the context switch process. Task A\u2019s state is restored, and it begins executing from the point where it left off.</p> </li> </ul>"},{"location":"3-resources/linux/contex_switching/#key-points-in-the-context-switch-process","title":"Key Points in the Context Switch Process","text":"<ol> <li>State Saving and Restoring: The operating system saves the current state (CPU registers, program counter, stack pointer) of the running task and restores the saved state of the new task.</li> <li>Task Isolation and Security: The kernel isolates each task\u2019s memory and state to ensure security. Virtual memory and memory protection mechanisms prevent tasks from interfering with each other.</li> <li>Efficient Task Scheduling: The kernel uses algorithms to efficiently decide which task to run next. The goal is to minimize the overhead of context switching, so the CPU spends more time running tasks and less time switching between them.</li> <li>Memory Management: Each task operates in its own virtual address space, and the kernel manages memory so that one task cannot corrupt the memory of another task.</li> <li>Preemptive Multitasking: The kernel uses preemptive multitasking, meaning the scheduler can interrupt a running task and switch to another at any point. This enables fair sharing of the CPU and ensures that tasks with higher priority or urgency get to run when needed.</li> </ol>"},{"location":"3-resources/linux/contex_switching/#in-summary","title":"In Summary","text":"<p>A context switch is a fundamental mechanism that allows the kernel to efficiently manage multiple tasks running on a single CPU. The process involves saving the current task\u2019s state, selecting the next task, and restoring its state so that it can continue running. This switch happens quickly and securely, with mechanisms in place to ensure that tasks are isolated from each other to prevent interference. The kernel ensures that context switches are done efficiently to minimize the overhead and ensure the system can run many tasks simultaneously.</p> <p>I cannot provide direct links, but I can suggest some online resources that commonly have useful diagrams, visualizations, and flowcharts related to context switching. You can visit the following websites and look for related content:</p> <ol> <li>Linux Kernel Documentation    The official Linux kernel documentation has detailed descriptions of process management, including context switching.</li> <li> <p>Link: https://www.kernel.org/doc/html/latest/</p> </li> <li> <p>Operating Systems: Three Easy Pieces    This book, available for free online, is an excellent resource for understanding operating system concepts, including process scheduling and context switching. The book often provides diagrams and flowcharts to explain key concepts.</p> </li> <li> <p>Link: https://pages.cs.wisc.edu/~remzi/OSTEP/</p> </li> <li> <p>GeeksforGeeks (GFG)    GeeksforGeeks provides tutorials on operating system concepts like process management and context switching. They often include flowcharts and diagrams to explain these topics.</p> </li> <li> <p>Link: https://www.geeksforgeeks.org/</p> </li> <li> <p>Wikipedia (Context Switching)    Wikipedia articles often include diagrams and visual aids along with detailed explanations of operating system concepts. The article on context switching usually includes relevant diagrams.</p> </li> <li> <p>Link: https://en.wikipedia.org/wiki/Context_switch</p> </li> <li> <p>Visualgo    Visualgo provides visualizations for many computer science concepts, including scheduling and process management, which can help illustrate how context switching works.</p> </li> <li> <p>Link: https://visualgo.net/en</p> </li> <li> <p>YouTube    Many YouTube channels focused on computer science and operating systems have animated explanations of context switching with accompanying diagrams. Search for terms like \"context switching animation\" or \"operating system context switch.\"</p> </li> <li> <p>OSDev Wiki    The OSDev Wiki contains detailed articles about operating system development, including context switching. It sometimes includes diagrams and visual explanations to help understand how the kernel manages processes.</p> </li> <li>Link: https://wiki.osdev.org/Main_Page</li> </ol> <p>By visiting these resources, you'll find plenty of diagrams and additional materials that explain context switching in an easily understandable way.</p>"},{"location":"3-resources/linux/networking/","title":"Networking","text":"<p>[[Linux]]</p>"},{"location":"3-resources/linux/networking/#etchosts-file","title":"/etc/hosts file","text":"<ul> <li>ip-hostname pairs</li> <li>list of ip addresses and hostnames - provides hostname resolution to an ipv4 or ipv6 address</li> <li>built-in system dns bypass, has maximum priority</li> <li>A match found in the /etc/hosts file will be used before any DNS entry. If the name searched is found in the file (like localhost), no DNS resolution is performed at all as the IP is known.</li> </ul> etc/hosts<pre><code>192.168.1.15 server-a\n192.168.1.16 server-b\n192.168.1.17 server-c\n192.168.1.18 server-d\n</code></pre>"},{"location":"3-resources/linux/networking/#resolvconf","title":"Resolv.conf","text":"<ul> <li>/etc/resolv.conf specifies the nameservers used for DNS resolution by the host. If you are using DHCP, this file is automatically populated with DNS record issued by DHCP server.</li> <li>It specifies the nameservers in order of search preference for resolver lookups, where it will actually use the DNS protocol for resolving the hostnames.</li> </ul> /etc/resolv.conf<pre><code>nameserver 127.0.0.53\noptions edns0 trust-ad\nsearch .\n</code></pre>"},{"location":"3-resources/linux/networking/#etcnsswitchconf","title":"/etc/nsswitch.conf","text":"<ul> <li>name services switch</li> <li>Lists the order of sources that must be used by various system library lookup functions. For example, <code>hosts</code> sets the order of hostname resolution to <code>files (aka - /etc/hosts/)</code> then <code>dns (aka - /etc/resolv.conf)</code>.</li> </ul> /etc/nsswitch.conf<pre><code>passwd:         files systemd sss\ngroup:          files systemd sss\nshadow:         files systemd sss\ngshadow:        files systemd\n\nhosts:          files mdns4_minimal [NOTFOUND=return] dns mymachines\nnetworks:       files\n</code></pre>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_13_address_spaces/","title":"Ch 13 address spaces","text":"<p>[[Linux]] [[ostep]]</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_13_address_spaces/#ch-13-address-spaces","title":"ch 13 address spaces","text":"<p>Goal is to leave proceses in memory while switching between them in order to allow the OS to implement time sharing efficiently. Don't save memory to disk, it is too slow.</p> <p>Address space is an easy to use abstraction of physical memory that the OS creates in conjunction with the hardware.</p> <p>The address space contains all the memory state of the running program. The code/instructions, stack to track function call chain and local vars, and the heap, used for user managed dynamically allocated memory (malloc'ed), all exists in the address space.  Stack grows up from the limit of the address space, heap grows down.</p> <p>OS has to make sure that when a program tries to load an instruction at address 0, it does not go to the actual physical address 0 but translates the virtual address 0 to some arbitrary address</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/","title":"Ch 2 introduction to operating systems","text":"<p>[[Linux]] [[ostep]]</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#chapter-2-introduction-to-operating-systems","title":"Chapter 2 Introduction to Operating Systems","text":"<p>A running program executes instructions.</p> <p>When a program runs, millions of times each second, the processor fectches instructions from memory, decodes it and executes it until the program completes.</p> <p>The opreating system is the software that manages the system/hardware correctly and efficiently in an easy to use manner.</p> <p>The OS does this through a technique called virtualization. The OS provides an interface/API to allow users to tell the OS what to do. Every OS has several hundred system calls  to run programs, access memory, devices, and files.</p> <p>Because virtualization allows many programs to run concurrently and access their own instructions and data, the OS is also known as a resource manager.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#virtualizing-the-cpu","title":"Virtualizing the CPU","text":"<p>Turning a single CPU into the illusion that the system has a large number of virtual cpus allowing multiple programs to run at once is one of the primary jobs of the operating system.</p> <p>Policies are the way the operating system decides the priority of which program to run when and how to allocate resources.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#virtualizing-memory","title":"Virtualizing Memory","text":"<p>Physical memory is the actual RAM in the system. Memory is an array of bytes, to read memory one must specify the address to access the data stored there. To write or update memory, one must also specify the data to be written to the address.</p> <p>Memory is accessed all the time the program is running. A running program keeps all of its data structures in memory. Each instruction of the program is stored in memory too, so memory is accesed on each instruction fetch.</p> <p>Each running program appears to have its own private memory instead of sharing the same physical memory with all other prorams. This is done through an OS technique called virtualizing memory. Each process accesses its own private virtual address space which the OS maps onto the physical memory of the machine. Physical memory is a shared resource that is managed by the OS.</p> <p>Concurrency - refers to the host of problems that arise and must be addressed when working on many things at once (concurrently) in the same program. Concurrency is how the OS juggles many things at once, first running one process then another.</p> <p>Threads - a function of a program that runs in the same memory space as other functions, with more than one active at a time. When many threads are working in the same memory space all at once how can we build a correctly working program?  This is problem of concurrency.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#persistence","title":"Persistence","text":"<p>System memory like RAM stores values in a volatile manner - when power goes away or the system crashes the data in memory is lost. We need hardware and software to store data persistently.</p> <p>The hardware comes in the form of input/output (I/O) devices, a hard drive or ssd for example.</p> <p>The operating system sofware that manages the disk is called the file system. It stores files the user creates on disk. The file system is the part of the OS responsible for managing persistent data.</p> <p>System calls to save a file include <code>open()</code> which opens the file and creates it, <code>write()</code> to write data to the file, and <code>close()</code> to close the file. These system calls are routed to the file system which handles the requests and returns a error code to the user.</p> <p>Device drivers are how the OS actually wites to the disk. The OS provides a standard way to access devices through systems calls.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#design-goals","title":"Design goals","text":"<p>The OS takes physical resources, vitualizes them, handles tricky issues related to concurrency, and stores files persistently and safe to access over the long term.</p> <p>Finding the right tradeoffs in these areas is the key to building an operating system. One of the goals in designing and implementing an operating system is to provide high performance and minimize overhead. Overhead comes in many forms - extra time, more instructions, extra space in memory or disk.</p> <p>Isolation is one of the main principles of operating systems. Isolating processes from one another and the operating system in general and is the key to protection.</p> <p>Code that runs on behalf of the OS is special. It has control of devices and should be treated differently than normal application code.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#system-calls","title":"System calls","text":"<p>The idea of system calls is to provide a special pair of hardware instructions and hardware state to make a controlled process. The key differnece between a system call and a procedure call is that a system call transfers control (i.e. jumps) into the kernel space while simlutaneously rasing the hardware privilege level. User applications run in user mode, which means the hardware restricts what applications can do - an application typically cannot initiate an I/O request to disk.  When a system call is initiated a special hardware instruction called a trap.  The hardware transfers control to a pre-specified trap hanlder and raises the privlege level to kernel model. In kernel mode the OS has full access to the hardware and can do things like I/O, network requests, or issue more memory to a program. When the OS is done with the request, it passes control back to the user via return from trap instructions that revert to user mode and pass control back to where the applcation left off.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_2_introduction_to_operating_systems/#multiprogramming","title":"Multiprogramming","text":"<p>Multiprogramming is a way to make better use of machine resources. Insteafd of just running one job at a time, the OS would load multiple jobs into memory and switch rapidly between them, improving CPU utilization. Switching is important because I/O devices are slow and having a program wait on CPU while an I/O request was serviced wasted prescious CPU time.  Memory protection keeps programs from accessing one anothers memory. Concurrency issues while jobs were waiting and resuming in the presence of interrupts led to developments in scheduling.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_4_the_process/","title":"Ch 4 the process","text":"<p>[[Linux]] [[ostep]]</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_4_the_process/#the-process","title":"The Process","text":"<p>A process is a running program. A program is a bunch of instructions on disk. The operating system gets these running and transforms the program into something useful.</p> <p>How does the OS provide the illusion of many CPUs?</p> <p>The OS creates the illusion of many CPUs by virtualizing the CPU. By running one process, then quickly stopping and running another, the OS can create an illusion that dozens of CPUs exist when in fact there is one (or a few). This technique is known as time sharing the CPU.</p> <p>Mechanisms are the low-level methods and protocols that implement a piece of functionality. For example, a context switch gives the OS the ability to stop running one program and start running another on a given CPU.</p> <p>On top of mechanisms, policies are the intelligence algorithms for making some kind of decision in the OS. A scheduling policy decides which program to run from a list of potentially runnable programs.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_4_the_process/#the-abstraction-of-a-running-program-a-process","title":"The abstraction of a running program: A process","text":"<p>The abstraction the OS provides of a running program is called a process. To understand a process we have to understand what constitutes its state at any point in time.</p> <p>One important component of state include its machine state, which is made up of memory (i.e. address space), CPU registers and I/O information (i.e. which file are open).  Special CPU registers include the program counter (i.e. instruction pointer), stack pointer and frame pointer are used to manage th stack (points to function parameters, address spaces, and local variables).</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_4_the_process/#the-process-api","title":"The process API","text":"<p>Process APIs include system calls to create and destroy processes and manage them. Examples include <code>fork()</code>, <code>exec()</code>, <code>wait()</code> and <code>exit()</code>. Other management tasks include getting status information and other controls such as suspending and resuming the process.</p> <p>Process creation</p> <ol> <li>The first thing the OS must do is load its code and static data from disk into memory in the address space of the process. Modern OSes perform this task lazily, meaning that the code is loaded only as they are needed during program execution.</li> <li>The OS must allocate memory for the program's stack and heap. The stack is used to manage function calls and local variables, while the heap is used for dynamically allocated memory (e.g., memory allocated with <code>malloc()</code> and freed with <code>free()</code>, data structures like linked lists, hash tables, and trees).</li> <li>Each process has three open file descriptors for standard input, standard output and standard error. The OS must set these up so that the process can read input from the terminal and write output to the screen.</li> <li>Finally the OS will start the program running at the entry point <code>main()</code> and the transfer control of the CPU to the process to begin execution.</li> <li>The OS will also set up the process control block (PCB) which contains all the information about the process, including its state, memory usage, open files, and other resources.</li> </ol>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_4_the_process/#process-states","title":"Process states","text":"<p>A process can be in one of several different states at a given time:</p> <ul> <li>Running: a process is running on the processor and executing instructions</li> <li>Ready: A process is ready to run, but the OS is choosing not to run it at this moment.</li> <li>Blocked: A process has performed some kind of operation that makes it not ready until another event takes place. For example, when a process initiates I/O requests to disk, it is blocked and another process can use the processor. Once I/O completes (or some other event causing blocking, like waiting on a network packet) the process is moved to Ready.</li> </ul> <p>Scheduling: Being moved from ready to running means the process has been scheduled. Moved from running to ready means the process has been descheduled. Decisions on which processes to run and when are made by the Scheduler.</p>"},{"location":"3-resources/linux/operating_systems_three_easy_pieces/ch_4_the_process/#data-structures","title":"Data Structures","text":"<p>The OS has key data structures that track various pieces of process information, such as the state of each process. The OS keeps a process list (aka task list) for all ready processes, blocked processes, When a processes is blocked the contents of the process's registers must be saved to memory. When the process resumes the registers are restored by moving them from memory to the physical registers on the CPU so the process resumes. This is called a context switch.</p> <p>Process control block (PCB): The structure that stores information about a given process (program counter, stack pointer, PID etc)</p> <p></p>"},{"location":"3-resources/python/Claude%20-%20Build%20a%20weather%20MCP%20server/","title":"Claude   Build a weather MCP server","text":"<p>[[mcp]] [[claude]] [[python]]</p> <p>https://modelcontextprotocol.io/docs/develop/build-server</p> <p>Get started building your own server to use in Claude for Desktop and other clients. In this tutorial, we\u2019ll build a simple MCP weather server and connect it to a host, Claude for Desktop. \u200b What we\u2019ll be building We\u2019ll build a server that exposes two tools: get_alerts and get_forecast. Then we\u2019ll connect the server to an MCP host (in this case, Claude for Desktop):</p>"},{"location":"3-resources/python/Pytest%20-%20logging%20and%20capturing%20stdout%20and%20stderr/","title":"Pytest   logging and capturing stdout and stderr","text":"<p>[[python]] [[pytest]] [[testing]] [[unit testing]] [[logging]]</p>"},{"location":"3-resources/python/Pytest%20-%20logging%20and%20capturing%20stdout%20and%20stderr/#caplog-fixture","title":"caplog fixture","text":"<p>https://docs.pytest.org/en/stable/how-to/logging.html</p> <p>Inside tests it is possible to change the log level for the captured log messages. This is supported by the <code>caplog</code> fixture:</p> <pre><code>def test_foo(caplog):\n    caplog.set_level(logging.INFO)\n</code></pre> <p>Lastly all the logs sent to the logger during the test run are made available on the fixture in the form of both the <code>logging.LogRecord</code> instances and the final log text. This is useful for when you want to assert on the contents of a message:</p> <pre><code>def test_baz(caplog):\n    func_under_test()\n    for record in caplog.records:\n        assert record.levelname != \"CRITICAL\"\n    assert \"wally\" not in caplog.text\n</code></pre>"},{"location":"3-resources/python/Pytest%20-%20logging%20and%20capturing%20stdout%20and%20stderr/#-capture","title":"-- capture","text":"<p>https://docs.pytest.org/en/stable/how-to/capture-stdout-stderr.html</p> <p>The <code>--capture=</code> command line flag intercepts stdout and stderr. The flag configures reporting, but a pytest fixture can also be used for more granular control.</p> <pre><code>pytest -s                  # disable all capturing\n</code></pre>"},{"location":"3-resources/python/Python%20Asyncio/","title":"Python Asyncio","text":"<p>[[python]] [[async]] [[concurrency]]</p> <p>Python\u2019s asyncio library enables you to write concurrent code using the async and await keywords. The core building blocks of async I/O in Python are awaitable objects\u2014most often coroutines\u2014that an event loop schedules and executes asynchronously. This programming model lets you efficiently manage multiple I/O-bound tasks within a single thread of execution.</p> <p>In this tutorial, you\u2019ll learn how Python asyncio works, how to define and run coroutines, and when to use asynchronous programming for better performance in applications that perform I/O-bound tasks.</p> <p>By the end of this tutorial, you\u2019ll understand that:</p> <pre><code>Python\u2019s asyncio provides a framework for writing single-threaded concurrent code using coroutines, event loops, and non-blocking I/O operations.\nFor I/O-bound tasks, async I/O can often outperform multithreading\u2014especially when managing a large number of concurrent tasks\u2014because it avoids the overhead of thread management.\nYou should use asyncio when your application spends significant time waiting on I/O operations, such as network requests or file access, and you want to run many of these tasks concurrently without creating extra threads or processes.\n</code></pre> <p>https://realpython.com/async-io-python/</p>"},{"location":"3-resources/python/Python%20MCP%20Server%20for%20LLMs/","title":"Python MCP Server for LLMs","text":"<p>[[python]] [[mcp]] [[ai agents]] [[llm]] [[ai tools]]</p> <p>Make an MCP Server to build tools for AI agents</p> <p>The Model Context Protocol (MCP) is a new open protocol that allows AI models to interact with external systems in a standardized, extensible way. In this tutorial, you\u2019ll install MCP, explore its client-server architecture, and work with its core concepts: prompts, resources, and tools. You\u2019ll then build and test a Python MCP server that queries e-commerce data and integrate it with an AI agent in Cursor to see real tool calls in action.</p> <p>By the end of this tutorial, you\u2019ll understand:</p> <pre><code>What MCP is and why it was created\nWhat MCP prompts, resources, and tools are\nHow to build an MCP server with customized tools\nHow to integrate your MCP server with AI agents like Cursor\n</code></pre> <p>You\u2019ll get hands-on experience with Python MCP by creating and testing MCP servers and connecting your MCP to AI tools. To keep the focus on learning MCP rather than building a complex project, you\u2019ll build a simple MCP server that interacts with a simulated e-commerce database. You\u2019ll also use Cursor\u2019s MCP client, which saves you from having to implement your own.</p> <p>https://realpython.com/python-mcp/</p>"},{"location":"3-resources/python/Resy%20Bot%20-%20Basic%20reservation%20scraper%20with%20selenium/","title":"Resy Bot   Basic reservation scraper with selenium","text":"<p>[[python]] [[bots]] [[resy reservations]] [[selenium]]</p> <p>https://github.com/jaredcrace/cracecasts/blob/main/examples/reservation_scraper/reservation_scraper.py</p> <ul> <li>Scrapes open reservations and sends alerts to slack with times available. You manually go and reserve the times</li> </ul>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/","title":"Resy Bot reservation scraper","text":"<p>[[python]] [[bots]] [[resy reservations]]</p> <p>https://www.toolpioneers.com/post/restaurant-reservation-automation-bot-retool-workflows</p> <p>Restaurant Reservation Automation Bot with Retool Workflows for a Concierge Services Company</p> <p>A white glove service company was tired of manually reserving tables for a large base of customers at the best fine dining restaurants in the United States. We built a game changing solution for them: an automated bot that books restaurant reservations using Retool and the RESY API. Let's dive into how it works.</p> <p></p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-1-obtaining-the-resy-api-key","title":"Step 1: Obtaining the RESY API Key","text":"<p>To start, you need an API key from RESY. This requires creating an account on RESY. Once logged in, open the console log, interact with any API endpoint, and locate the authorization header. Here, you'll find your much-needed API key.</p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-2-user-input-collection","title":"Step 2: User Input Collection","text":"<p>User preferences are gathered through an Excel sheet. This includes:</p> <ul> <li> <p>Party size</p> </li> <li> <p>Desired day</p> </li> <li> <p>Venue ID</p> </li> <li> <p>Preferred time (preferred time within specified time interval)</p> </li> <li> <p>Minimum time slot (starting of the specified time interval)</p> </li> <li> <p>Maximum time slot (ending of the specified time interval)</p> </li> </ul> <p>Latitude and longitude are set as default parameters (0,0).</p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-3-time-preferences","title":"Step 3: Time Preferences","text":"<p>Users indicate their minimum and maximum time slots, representing the range within which they wish to book a table. The \"preferred time\" is the specific time slot desired within this range. If the preferred time isn't available, the bot selects the next available slot within the specified interval.</p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-4-the-cron-job","title":"Step 4: The Cron Job","text":"<p>The bot operates on a cron job set to run every five minutes. It retrieves user inputs from the Excel sheet and starts processing.</p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-5-fetch-venue-details","title":"Step 5: Fetch Venue Details","text":"<p>A GET request fetches venue details, yielding multiple config IDs. These IDs represent different reservation time slots, which are parsed using regex.</p> <p></p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-6-filtering-time-slots","title":"Step 6: Filtering Time Slots","text":"<p>A JavaScript query block filters all available reservation slots, matching them with user-defined minimum and maximum times. It then returns the config ID for the slot closest to the preferred time.</p> <p></p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-7-booking-process","title":"Step 7: Booking Process","text":"<p>Upon obtaining the correct config ID, the bot hits a new endpoint to fetch a booking token. This token is essential for the next step.</p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-8-making-the-reservation","title":"Step 8: Making the Reservation","text":"<p>The bot sends a POST request as form data, carrying the booking token and other necessary data. This step either confirms or denies the reservation.  </p> <p></p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#step-9-updating-the-excel-sheet","title":"Step 9: Updating the Excel Sheet","text":"<p>If the reservation is successful, the bot updates the Excel sheet, and the cron job for the particular reservation request stops, while reservation requests for other clients happen parallelly.  </p>"},{"location":"3-resources/python/Resy%20Bot%20reservation%20scraper/#conclusion","title":"Conclusion","text":"<p>This automation bot built using Retool workflows, significantly streamlines the process of making restaurant reservations. By efficiently handling user preferences and navigating through the RESY API, it ensures a hassle-free booking experience. Perfect for those who love dining out but dislike the tedious reservation process!  </p> <p>Remember, this bot is a sophisticated tool that requires some technical know-how, especially in terms of API interaction and cron job setup. Nonetheless, its efficiency and effectiveness make it a valuable asset for regular diners and restaurant enthusiasts alike</p>"},{"location":"3-resources/python/Resy%20Grabber%20-%20Restaurant%20reservation%20bot/","title":"Resy Grabber   Restaurant reservation bot","text":"<p>[[python]] [[bots]] [[resy reservations]]</p> <p>This is an open-source tool to help manage restaurant reservations on Resy.com. It was previously a SaaS product but has been converted to a locally runnable application with no authentication required. This is the first SAAS product that I've released and it was very fun to work on! I am open sourcing this now though because New York passed laws making it illegal to sell dinner reservations.</p> <p>https://github.com/korbinschulz/resybot-open/tree/main</p> <ul> <li>I tested this bot, but it was failing due to some proxy issue even though it is supposed to be \"optional\"</li> <li>Tasks did not work as expected</li> <li>Easy enough to use</li> </ul>"},{"location":"3-resources/python/Type%20Checking%20in%20Python/","title":"Type Checking in Python","text":"<p>[[python]] [[type checking]] [[static testing]] [[testing]]</p> <p>Guide to type checking in Python</p> <p>Type hints in python help you to define what what objects should be (string, dict, list) and allow you to use tools that check your code to make sure it is consistent when it runs. </p> <p>In this tutorial, you\u2019ll learn about the following:</p> <ul> <li>Type annotations and type hints</li> <li>Adding static types to code, both your code and the code of others</li> <li>Running a static type checker</li> <li>Enforcing types at runtime https://realpython.com/python-type-checking/#function-annotations</li> </ul>"},{"location":"3-resources/system_design/kafka/","title":"Apache Kafka Overview","text":"<p>kafka design docs</p> <p>What is apache kafka?</p> <ul> <li>Apache Kafka is a distributed streaming platform that is used publish and subscribe to event streams.</li> <li>event streams - capture real-time data from event sources like databases, sensors, mobile devices, cloud services, and software application.</li> <li> <p>capabilities include captureing event streams, storing them durably for as long as needed, processing and reacting to events, and routing the event streams to different destinations.</p> </li> <li> <p>Event stream use cases:</p> </li> <li>payment processing and real time financial transactions, banks, stocks</li> <li>monitoring cars, trucks, fleets and shipments in logistics industry</li> <li>capture and analyze sensor data in real time from IOT devicies, factories</li> <li>Collect and react to customer interactions and orders in retail, hotel, travel</li> <li>Monitor patients in hospital care and predict patient outcomes</li> <li>Foundation for microservices, event driven architecture and real time analytics</li> </ul> <p>How does Kafka work?</p>"},{"location":"3-resources/system_design/kafka/#-it-is-a-distributed-system-consisting-of-servers-and-clients-that-communicate-via-a-custom-high-performance-tcp-network-protocol","title":"- It is a distributed system consisting of servers and clients that communicate via a custom high-performance TCP network protocol.","text":"<p>Components of Kafka:</p> <p>Servers:</p> <ul> <li>broker - Kafka server that stores data and serves clients</li> <li>zookeeper - Kafka uses Zookeeper to manage the cluster. Zookeeper is a distributed coordination service that Kafka uses to manage and coordinate brokers.</li> <li>controller - Kafka broker that is responsible for managing the state of partitions and replicas and for performing administrative tasks like reassigning partitions and electing leaders.</li> <li>kafka connect - server that imports and exports data as event streams to and from relationa databases and other systems.</li> <li>clients - integrate various languages with kafka</li> </ul> <p>Main Concepts:</p> <ul> <li>Event - an event records the fact something happened. It is also called a record or message. Kafka writes data in the form of events. Events have a key, value, timestamp, and metadata headers. Events are processed exactly once.</li> <li>Producer - applications that write/publish events to Kafka. The producer sends data directly to the broker that is the leader for the partition without any intervening routing tier. To help the producer do this all Kafka nodes can answer a request for metadata about which servers are alive and where the leaders for the partitions of a topic are at any given time to allow the producer to appropriately direct its requests.</li> <li>Consumer - applications that subscribe/read and process events. Producers and consumers are fully decoupled from each other. The Kafka consumer works by issuing \"fetch\" requests to the brokers leading the partitions it wants to consume. The consumer specifies its offset in the log with each request and receives back a chunk of log beginning from that position. The consumer thus has significant control over this position and can rewind it to re-consume data if need be. Every consumer has a consumer group.</li> <li>Broker - a kafka server/container that receives and stores messages from a producer and stores them durably. Brokers save data to disk. data is pushed to the broker from the producer and pulled from the broker by the consumer. One broker serves as the controller. Each broker hosts some set of partitions nd handles incoming requests to write new events to those partitions, read events, and handle replication between each other.</li> <li>Topic - events are stored in topics. Topics are like folders and events like files in a folder. Topics are multi-producer and multi-subscriber. Events in a topic can be read as often as needed. Unlike traditional message queues, events are not deleted after consumption, you define for how long Kafka should retain events via per-topic config settings. Old events are discarded. Storing data is fine, as performance is constant with respect to data size.</li> <li>Partition - Topics are divided into partitions, meaning it is spread over a number of buckets on different kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. To make your data fault-tolerant and highly-available, every topic can be replicated, even across geo-regions or datacenters, so that there are always multiple brokers that have a copy of the data just in case things go wrong, you want to do maintenance on the brokers, and so on. A common production setting is a replication factor of 3, i.e., there will always be three copies of your data. When a new event is published to a topic, it is actually appended to one of the topic's partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written.</li> <li> <p>Offset -  an offset is a unique identifier assigned to each record (message) within a partition of a Kafka topic. the Kafka offset represents the position of a message within that partition\u2019s log. It indicates how far the message is from the beginning of the partition log. Kafka uses offsets to track messages from initial writing to final processing completions.  Ofsets are stored persistently by Kafka, allowing consumers to resume from a specific point in the event of a failure. Every consumer notifies kafka by committing the offset information it has processed. Please note, offset is unique only within a partition, not across partitions. </p> </li> <li> <p>Zookeeper</p> </li> <li> </li> <li>Replication - Kafka replicates the log for each topic's partitions across a configurable number of servers (set by per topic replication factor). This allows automatic failover to replicas if a server in the cluster fails so messages remain available. The unit of replication is the topic partition. Each partition has a single leader and zero or more followers; the total numbmer of replicas including the leader consitute the replication factor.</li> <li>Leaders and followers in replication - In kafka, when replicating a topic's partitions, all writes go to the leader of the partition. Reads go to the leader or follower. There are many more partitions than brokers, and leaders are distributed evenly among brokers. Logs on the leader and follower are identical and have the same offsets and messages, but the leader may have a few unreplicated messages at the end of its log. Followers pull and consume messages from the leader and allows them to batch together log entries. During a failure, only members from the ISR are eligible for election as leader because the ISR (i.e. In sync replicas) is by definition caught up to the leader. A write to a Kafka partition is not considered committed until all in-sync replicas have received the write. This ISR set is persisted in the cluster metadata whenever it changes. Because of this, any replica in the ISR is eligible to be elected leader.</li> </ul>"},{"location":"3-resources/system_design/kafka/#consumer-group-a-consumer-group-is-a-set-of-consumers-that-consume-from-the-same-topic-kafka-ensures-that-a-partition-is-assigned-to-only-one-consumer-at-a-time-this-helps-kafka-remove-the-complexities-of-sharing-messages-within-a-single-partition-with-multiple-consumers-the-only-remaining-problem-is-ensuring-the-consumers-get-reliable-data-from-the-assigned-partitions-this-is-where-offsets-come-into-the-picture-kafka-uses-offsets-to-track-messages-from-initial-writing-to-final-processing-completions","title":"Consumer Group - A consumer group is a set of consumers that consume from the same topic. Kafka ensures that a partition is assigned to only one consumer at a time. This helps Kafka remove the complexities of sharing messages within a single partition with multiple consumers. The only remaining problem is ensuring the consumers get reliable data from the assigned partitions. This is where offsets come into the picture. Kafka uses offsets to track messages from initial writing to final processing completions.","text":""},{"location":"3-resources/system_design/kafka/#-controller-a-special-node-in-kafka-that-is-responsible-for-managing-the-registration-of-brokers-in-the-cluster-in-order-to-be-considered-active-a-broker-must-maintain-an-active-session-with-the-controller-to-receieve-meta-data-updates-an-active-session-is-often-maintained-via-heartbeat-checks-and-brokers-acting-as-followers-must-replicate-writes-from-the-leader-and-not-fall-too-far-behind-nodes-satisfying-these-conditions-are-considered-in-sync-and-referred-to-as-the-isr-in-sync-replicas-when-a-follower-dies-or-falls-too-far-behind-ie-replica-lag-time-exceeds-max-configuration-it-is-removed-from-the-isr","title":"- Controller - A special node in kafka that is responsible for managing the registration of brokers in the cluster. In order to be considered \"Active\" - a Broker must maintain an active session with the controller to receieve meta data updates (an active session is often maintained via heartbeat checks) and brokers acting as followers must replicate writes from the leader and not fall too far behind. Nodes satisfying these conditions are considered \"in sync\" and referred to as the ISR (in sync replicas). when a follower dies or falls too far behind (.i.e. replica lag time exceeds max configuration), it is removed from the ISR.","text":"<ul> <li>Messages are considered commited when all replicas in the ISR for that partition have applied it to their log. Only commited messages are given out to the consumer, consumers don't need to worry about missing messages if a leader fails.</li> <li>Producers can wait for messages to be commited or not depending on their trade off for latency and durabilty, as controlled by the acks setting the producer users. Kafka guarantees that a committed message will not be lost, as long as there is at least one in sync replica alive, at all times.</li> </ul> <p>-</p> <p>Problems:</p> <ul> <li>consumer lag - One of Kafka's key performance indicators is consumer lag. It represents the difference between the committed offset and the log-end offset. A minimal lag between log-end offset and committed offset is expected during normal operations. However, if the lag increases, it may break the system. The most common reason for high lag is unpredictable surges in incoming messages, as well as uneven data distribution across partitions, and slow processing jobs.</li> <li>Kafka splits data into partitions by considering the hash of the message key. If you customize the message key and the message volume with a specific key is higher than others, the consumer catering to that partition experiences a high load, leading to high lag.</li> </ul> <p>What if all the replicas and leaders die?</p> <ul> <li>This is a simple tradeoff between availability and consistency. If we wait for replicas in the ISR, then we will remain unavailable as long as those replicas are down. If such replicas were destroyed or their data was lost, then we are permanently down. If, on the other hand, a non-in-sync replica comes back to life and we allow it to become leader, then its log becomes the source of truth even though it is not guaranteed to have every committed message. By default from version 0.11.0.0, Kafka chooses the first strategy and favor waiting for a consistent replica.</li> </ul> <p>Look over log compaction then done: 4.8: https://kafka.apache.org/documentation/#design</p>"},{"location":"4-archive/dump/mkdocs_notes/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"4-archive/dump/mkdocs_notes/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"4-archive/dump/mkdocs_notes/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files\n</code></pre> <p>Good links: Feature overview material for mkdocs: https://jameswillett.dev/getting-started-with-material-for-mkdocs/#share-on-socials</p> <p>Material for MKdocs: https://squidfunk.github.io/mkdocs-material/getting-started/</p>"},{"location":"4-archive/dump/prompts/","title":"Useful gpt prompts","text":""},{"location":"4-archive/dump/prompts/#code-review","title":"code review","text":"<p>Provide a code review for my blackjack game. Offer input or suggestions for improvment. Explain what to improve, how to improve it, and why it is incorrect/inefficient. </p>"}]}